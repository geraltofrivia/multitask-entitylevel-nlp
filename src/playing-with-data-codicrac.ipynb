{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly, we turn the CONLL files into a (pickled) list of custom dataclass objects.\n",
    "The dataclass is found in src/utils/data.py. I would recommend going over them in case of any problems or contacting Priyansh :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is what the ideal directory tree looks like\n",
    "\n",
    "```sh\n",
    "priyansh@priyansh-ele:~/Dev/research/coref/mtl$ tree -L 4\n",
    ".\n",
    "├── data\n",
    "│   ├── linked\n",
    "│   ├── manual\n",
    "│   │   ├── ner_ontonotes_tag_dict.json\n",
    "│   │   ├── ner_scierc_tag_dict.json\n",
    "│   │   ├── rel_scierc_tag_dict.json\n",
    "│   │   └── replacements.json\n",
    "│   ├── parsed\n",
    "│   │ <...> \n",
    "│   ├── raw\n",
    "│   │   ├── codicrac-ami\n",
    "│   │   │   └── AMI_dev.CONLLUA\n",
    "│   │   ├── codicrac-arrau\n",
    "│   │   │   └── ARRAU2.0_UA_v3_LDC2021E05.zip\n",
    "│   │   ├── codicrac-arrau-gnome\n",
    "│   │   │   └── Gnome_Subset2.CONLL\n",
    "│   │   ├── codicrac-arrau-pear\n",
    "│   │   │   └── Pear_Stories.CONLL\n",
    "│   │   ├── codicrac-arrau-rst\n",
    "│   │   │   ├── RST_DTreeBank_dev.CONLL\n",
    "│   │   │   ├── RST_DTreeBank_test.CONLL\n",
    "│   │   │   └── RST_DTreeBank_train.CONLL\n",
    "│   │   ├── codicrac-arrau-t91\n",
    "│   │   │   └── Trains_91.CONLL\n",
    "│   │   ├── codicrac-arrau-t93\n",
    "│   │   │   └── Trains_93.CONLL\n",
    "│   │   ├── codicrac-light\n",
    "│   │   │   ├── light_dev.CONLLUA\n",
    "│   │   │   ├── light_dev.CONLLUA.zip\n",
    "│   │   │   └── __MACOSX\n",
    "│   │   ├── codicrac-persuasion\n",
    "│   │   │   └── Persuasion_dev.CONLLUA\n",
    "│   │   ├── codicrac-switchboard\n",
    "│   │   │   ├── __MACOSX\n",
    "│   │   │   ├── Switchboard_3_dev.CONLL\n",
    "│   │   │   └── Switchboard_3_dev.CONLL_LDC2021E05.zip\n",
    "│   │   ├── ontonotes\n",
    "│   │   │   ├── conll-2012\n",
    "│   │   │   ├── ontonotes-release-5.0\n",
    "│   │   │   ├── ontonotes-release-5.0_LDC2013T19.tgz\n",
    "│   │   │   └── v12.tar.gz\n",
    "│   │   └── scierc\n",
    "│   │       ├── dev.json\n",
    "│   │       ├── sciERC_processed.tar.gz\n",
    "│   │       ├── test.json\n",
    "│   │       └── train.json\n",
    "│   └── runs\n",
    "│       └── ne_coref\n",
    "│           ├── goldner_all.json\n",
    "│           ├── goldner_some.json\n",
    "│           ├── spacyner_all.json\n",
    "│           └── spacyner_some.json\n",
    "├── g5k.sh\n",
    "├── models\n",
    "│ <...>\n",
    "├── preproc.sh\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── setup.sh\n",
    "├── src\n",
    "│   ├── analysis\n",
    "│   │   └── ne_coref.py\n",
    "│   ├── config.py\n",
    "│   ├── dataiter.py\n",
    "│   ├── eval.py\n",
    "│   ├── loops.py\n",
    "│   ├── models\n",
    "│   │   ├── autoregressive.py\n",
    "│   │   ├── embeddings.py\n",
    "│   │   ├── modules.py\n",
    "│   │   ├── multitask.py\n",
    "│   │   ├── _pathfix.py\n",
    "│   │   └── span_clf.py\n",
    "│   ├── _pathfix.py\n",
    "│   ├── playing-with-data-codicrac.ipynb\n",
    "│   ├── preproc\n",
    "│   │   ├── codicrac.py\n",
    "│   │   ├── commons.py\n",
    "│   │   ├── ontonotes.py\n",
    "│   │   ├── _pathfix.py\n",
    "│   │   └── scierc.py\n",
    "│   ├── run.py\n",
    "│   ├── utils\n",
    "│   │   ├── data.py\n",
    "│   │   ├── exceptions.py\n",
    "│   │   ├── misc.py\n",
    "│   │   ├── nlp.py\n",
    "└── todo.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on the `data/raw` folder. Specially the CODICRAC subfolders. This is how we want the raw data to exist like.\n",
    "We can thus proceed to preprocess them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Preprocess raw files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python src/preproc/codicrac.py -a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the command above should do it for you. But if not, you would want to invoke the CODICRACParser class with the right path.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from preproc.codicrac import CODICRACParser\n",
    "path_to_conll_file = Path('../data/raw/codicrac-light')\n",
    "parser = CODICRACParser(path_to_conll_file)\n",
    "\n",
    "parser.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the outputs might have been written to `../data/parsed/codicrac-light`.\n",
    "By default, everything is stored in the `data/parsed` directory. \n",
    "But you can change that, if you want, by specifying the `write_dir` argument. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing paths from /home/priyansh/Dev/research/coref/mtl/src\n",
      "Successfully written 20 at ../data/parsed/codicrac-light.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from preproc.codicrac import CODICRACParser\n",
    "\n",
    "path_to_conll_file = Path('../data/raw/codicrac-light')\n",
    "parser = CODICRACParser(path_to_conll_file, write_dir='../potato/tomato/basilic')\n",
    "\n",
    "parser.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: you can also have a look at the parsers declared in the `run` function at the end of `src/preproc/codicrac.py` to get an idea of how easily preproc datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PreProcessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataiter import DocumentReader\n",
    "dr = DocumentReader(src=\"codicrac-light\")\n",
    "len(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('light_dev/episode_678',\n",
       " [[[0, 2], [12, 14], [87, 89]], [[2, 4]], [[4, 6]]],\n",
       " [['a', 'candle'], ['a', 'wall'], ['a', 'temple']],\n",
       " ['concrete', 'concrete', 'concrete'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing this data by index\n",
    "dr[4].docname, dr[4].coref.spans[:3], dr[4].ner.words[:3], dr[4].ner.tags[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light_dev/episode_6686\n",
      "light_dev/episode_5399\n",
      "light_dev/episode_6557\n"
     ]
    }
   ],
   "source": [
    "# Accessing this data by loop\n",
    "for i, instance in enumerate(dr):\n",
    "    \n",
    "    if i > 2: break\n",
    "    \n",
    "    print(instance.docname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "7\n",
      "20\n",
      "11\n",
      "16\n",
      "98\n",
      "5\n",
      "20\n",
      "335\n",
      "18\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# Let's try and load every dataset\n",
    "print(DocumentReader(src=\"codicrac-persuasion\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-ami\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-light\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-switchboard\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-t91\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-t93\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-gnome\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-pear\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-rst\", split=\"train\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-rst\", split=\"dev\").__len__())\n",
    "print(DocumentReader(src=\"codicrac-arrau-rst\", split=\"test\").__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What processed data looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of `Document` instances (src/utils/data.py).\n",
    "Each document has the following fields:\n",
    "\n",
    "**document**: `List[List[str]]`: A list of sentences where each sentence itself is a list of strings. For instance: \n",
    "\n",
    "```py\n",
    "[\n",
    "    [\"I\", \"see\", \"a\", \"little\", \"silhouette\", \"of\", \"a\", \"man\"], \n",
    "    [\"Scaramouche\", \"Scaramouche\", \"will\", \"you\", \"do\", \"the\", \"Fandango\"]\n",
    "]\n",
    "```\n",
    "\n",
    "**pos**: `List[List[str]]`: The same as above except every string is replaced by its POS tag. \n",
    "Warning: this is not an optional field. So in case your document is not annotated with pos tags, you can pass fake pos tags (and choose to not exercise them down the line). You can do this simply by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE'],\n",
      " ['FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE']]\n",
      "Corresponding to\n",
      "[['I', 'see', 'a', 'little', 'silhouette', 'of', 'a', 'man'],\n",
      " ['Scaramouche', 'Scaramouche', 'will', 'you', 'do', 'the', 'Fandango']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from utils.data import Document\n",
    "doc_text = [\n",
    "    [\"I\", \"see\", \"a\", \"little\", \"silhouette\", \"of\", \"a\", \"man\"], \n",
    "    [\"Scaramouche\", \"Scaramouche\", \"will\", \"you\", \"do\", \"the\", \"Fandango\"]\n",
    "]\n",
    "fake_pos = Document.generate_pos_tags(doc_text)\n",
    "pprint(fake_pos)\n",
    "print(\"Corresponding to\")\n",
    "pprint(doc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**docname**: str\n",
    "\n",
    "**genre**: str \n",
    "\n",
    "are both metadata fields that you can choose to use however you want. Ideally, docname should contain the docname. Genre can be left empty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coref**: Cluster\n",
    "    \n",
    "**ner**: NamedEntities\n",
    "    \n",
    "**bridging**: BridgingAnaphor\n",
    "    \n",
    "**rel**: TypedRelations\n",
    "    \n",
    "are the fields which contain task specific annotations.\n",
    "All these four things are represented with their custom data classes (also found in `src/utils/data.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster (Coreference Annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primarily, it consists of a list of spans (indices). \n",
    "These indices correspond to the Document.document field (see \"I see ... fandango\" snippet above). \n",
    "So, let's imagine our document looks something like:\n",
    "\n",
    "```py\n",
    "# I saw a dog in a car. It was really cute. Its skin was brown with white spots !\n",
    "doc = [\n",
    "    [\"I\", \"saw\", \"a\", \"dog\", \"in\", \"a\", \"car\", \".\"],                # 8 tokens\n",
    "    [\"It\", \"was\", \"really\", \"cute\", \".\"],                           # 5 tokens\n",
    "    [\"Its\", \"skin\", \"was\", \"brown\", \"with\", \"white\", \"spots\", \"!\"]  # 8 tokens\n",
    "] # total: 21 tokens\n",
    "```\n",
    "\n",
    "The clusters here would be <\"I\">, <\"a dog\", \"it\", and \"its\">, and <\"a car\">. That is, two singletons and one cluster with three spans. It would be represented by something like:\n",
    "\n",
    "```py\n",
    "clusters = [ \n",
    "    [[0, 1]],                        # one cluster with one span (about I) \n",
    "    [[2, 4], [8, 9], [13, 14]],      # next cluster with three spans (about the dog(\n",
    "    [[6, 8]]                         # last cluster (a car) with one span\n",
    "]\n",
    "```\n",
    "\n",
    "A span, by the way, is a list of two integers. `[6, 14]` is a span of tokens 6, 7, 8, 9, 10, 11, 12 and 13 (not 14; this is how python indexing works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I saw a dog in a car. It was really cute. Its skin was brown with white spots !\n",
    "doc = [\n",
    "    [\"I\", \"saw\", \"a\", \"dog\", \"in\", \"a\", \"car\", \".\"],                # 8 tokens\n",
    "    [\"It\", \"was\", \"really\", \"cute\", \".\"],                           # 5 tokens\n",
    "    [\"Its\", \"skin\", \"was\", \"brown\", \"with\", \"white\", \"spots\", \"!\"]  # 8 tokens\n",
    "] # total: 21 tokens\n",
    "\n",
    "clusters = [ \n",
    "    [[0, 1]],                        # one cluster with one span (about I) \n",
    "    [[2, 4], [8, 9], [13, 14]],      # next cluster with three spans (about the dog(\n",
    "    [[6, 8]]                         # last cluster (a car) with one span\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: you might notice that you can't directly access the text of a span by `doc[span[0]: span[1]]`. This is because the spans assume a flat list of strings. The document however contains structure based on sentences. So we need to flatten the document first. You can do that by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While doc has 3 items, each representing a sentence, a flattened doc has 21 items, each representing a token.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Its', 'skin', 'was', 'brown', 'with', 'white', 'spots']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.nlp import to_toks\n",
    "# to_toks (think of to_tokens)\n",
    "\n",
    "flattened_doc = to_toks(doc)\n",
    "print(f\"While doc has {len(doc)} items, each representing a sentence, \"\n",
    "      f\"a flattened doc has {len(flattened_doc)} items, each representing a token.\")\n",
    "\n",
    "span = [13, 20]\n",
    "flattened_doc[span[0]: span[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other helpful things in there as well such as\n",
    "**words**: replace each span `[13, 15]` with the corresponding list of tokens i.e. `['the', 'red', 'car']`.\n",
    "\n",
    "**pos**: replace every word with its POS tag\n",
    "\n",
    "In addition, corresponding to spans, words, and pos we also have **span_head**, **words_head**, and **pos_head** which contain the span's head information (as detected by spacy's span head detection algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Annotations\n",
    "\n",
    "### NamedEntities\n",
    "\n",
    "Similar to `Clusters` above except we don't need to aggregate groups of spans into clusters. So here, we just have a list of spans.\n",
    "\n",
    "`ner.spans: List[List[int]] = [ [2, 4], [4, 9] ... ]`\n",
    "\n",
    "Corresponding to each, we also have the NER tag, the words, the POS and the span head information as above.\n",
    "\n",
    "### BridgingAnaphors\n",
    "\n",
    "Each element of\n",
    "\n",
    "bridging.spans looks like: `[ [2, 5], [9, 13] ]`, i.e., a list of two spans. The first being the anaphor, and the second being the antecedent.\n",
    "\n",
    "You also have better variables at your disposal like `briding.anaphors` or `briding.antecedents` to access them more normally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}