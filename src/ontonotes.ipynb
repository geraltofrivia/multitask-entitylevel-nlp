{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Data\n",
    "\n",
    "\n",
    "Using code from [this mangoes notebook](https://gitlab.inria.fr/magnet/mangoes/-/blob/master/notebooks/BERT%20for%20Co-reference%20Resolution%20-%20Ontonotes.ipynb)\n",
    "\n",
    "Use [this page](https://cemantix.org/conll/2012/data.html) to get ontonotes in conll format. **Open it on chrome. not FF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from mangoes.modeling import BERTForCoreferenceResolution, MangoesCoreferenceDataset\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_word(word, language):\n",
    "    if language == \"arabic\":\n",
    "        word = word[:word.find(\"#\")]\n",
    "    if word == \"/.\" or word == \"/?\" or word == \"/-\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "\n",
    "def parse_document(path, language=\"english\"):\n",
    "    \"\"\"\n",
    "    Parses a single data file\n",
    "    Returns the data from whatever documents are in the file.\n",
    "    \n",
    "    returns:\n",
    "        words: Lists of Lists of Lists of strings. list of sentences. One sentence is a list of words.\n",
    "        cluster_ids: Lists of Lists of Lists of ints or tuple(ints). Words that aren't mentions have either -1 as id\n",
    "        speaker_ids: Lists of Lists of Lists of ints.\n",
    "        doc_keys: List of document keys\n",
    "    \"\"\"\n",
    "    doc_keys = []\n",
    "    doc_sents = []\n",
    "    doc_cluster_ids = []\n",
    "    doc_speaker_ids = []\n",
    "    sentences = []\n",
    "    sentence_cluster_ids = []\n",
    "    sentence_speaker_ids = []\n",
    "    cur_sentence_words = []\n",
    "    cur_sentence_cluster_ids = []\n",
    "    cur_sentence_speaker_ids = []\n",
    "    current_clusters = []\n",
    "    docs = 0\n",
    "    with open(path, \"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            if line.startswith(\"#begin document\"):\n",
    "                doc_key = line.split()[2][:-1]\n",
    "                doc_keys.append(doc_key[1:-1])\n",
    "                docs += 1\n",
    "            elif line.startswith(\"#end document\"):\n",
    "                assert len(sentences) == len(sentence_cluster_ids) == len(sentence_speaker_ids)\n",
    "                assert cur_sentence_words == []\n",
    "                doc_sents.append(sentences)\n",
    "                doc_cluster_ids.append(sentence_cluster_ids)\n",
    "                doc_speaker_ids.append(sentence_speaker_ids)\n",
    "                sentences = []\n",
    "                sentence_cluster_ids = []\n",
    "                sentence_speaker_ids = []\n",
    "            else:\n",
    "                data = line.split()\n",
    "                sentence_end = len(data) == 0\n",
    "                if sentence_end:\n",
    "                    sentences.append(cur_sentence_words)\n",
    "                    sentence_cluster_ids.append(cur_sentence_cluster_ids)\n",
    "                    sentence_speaker_ids.append(cur_sentence_speaker_ids)\n",
    "                    cur_sentence_words = []\n",
    "                    cur_sentence_cluster_ids = []\n",
    "                    cur_sentence_speaker_ids = []\n",
    "                else:\n",
    "                    cur_sentence_words.append(normalize_word(data[3], language))\n",
    "                    cur_sentence_speaker_ids.append(data[9])\n",
    "                    raw_cluster_id = data[-1]\n",
    "                    if raw_cluster_id == \"-\":\n",
    "                        if len(current_clusters) == 0:\n",
    "                            cluster_id = -1\n",
    "                        elif len(current_clusters) == 1:\n",
    "                            cluster_id = int(list(current_clusters)[0])\n",
    "                        else:\n",
    "                            cluster_id = tuple(int(item) for item in current_clusters)\n",
    "                    else:\n",
    "                        for part in raw_cluster_id.split(\"|\"):\n",
    "                            if \"(\" in part:\n",
    "                                current_clusters.append(part[1:-1] if \")\" in part else part[1:])\n",
    "                        if len(current_clusters) == 1:\n",
    "                            cluster_id = int(list(current_clusters)[0])\n",
    "                        else:\n",
    "                            cluster_id = tuple(int(item) for item in current_clusters)\n",
    "                        for part in raw_cluster_id.split(\"|\"):\n",
    "                            if \")\" in part:\n",
    "                                current_clusters.remove(part[1:-1] if \"(\" in part else part[:-1])\n",
    "                    cur_sentence_cluster_ids.append(cluster_id)\n",
    "        assert len(doc_sents) == docs\n",
    "        for i in range(docs):\n",
    "            doc_keys[i] += f\"_{i}\"\n",
    "        return doc_sents, doc_cluster_ids, doc_speaker_ids, doc_keys\n",
    "\n",
    "    \n",
    "def parse_dataset(path):\n",
    "    \"\"\"\n",
    "    Parses a directory of Ontonotes data files (train, dev, or test)\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "        words: Lists of Lists of Lists of strings. list of sentences. One sentence is a list of words.\n",
    "        cluster_ids: Lists of Lists of Lists of ints or tuple(ints). Words that aren't mentions have either -1 as id\n",
    "        speaker_ids: Lists of Lists of Lists of ints.\n",
    "        doc_keys: List of document keys.\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    dataset_sents = []\n",
    "    dataset_clusters = []\n",
    "    dataset_speakers = []\n",
    "    dataset_genres = []\n",
    "    dataset_doc_keys = []\n",
    "    for path in glob.iglob(path):\n",
    "        genre = path.split(\"/\")[-4]\n",
    "        doc_sents, doc_cluster_ids, doc_speaker_ids, doc_keys = parse_document(path)\n",
    "        assert len(doc_sents) == len(doc_cluster_ids) == len(doc_speaker_ids) == len(doc_keys)\n",
    "        dataset_sents += doc_sents\n",
    "        dataset_clusters += doc_cluster_ids\n",
    "        dataset_genres += [genre] * len(doc_sents)\n",
    "        dataset_doc_keys += doc_keys\n",
    "        for d in range(len(doc_speaker_ids)):\n",
    "            speakers = doc_speaker_ids[d]\n",
    "            speakers_to_ids = {speaker: i for i, speaker in\n",
    "                               enumerate(list(set([item for sublist in speakers for item in sublist])))}\n",
    "            for i in range(len(speakers)):\n",
    "                for j in range(len(speakers[i])):\n",
    "                    speakers[i][j] = speakers_to_ids[speakers[i][j]]\n",
    "            dataset_speakers.append(speakers)\n",
    "    gen_to_id = {g: i for i, g in enumerate(set(dataset_genres))}\n",
    "    return dataset_sents, dataset_clusters, dataset_speakers, dataset_genres, dataset_doc_keys, gen_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, train_clusters, train_speakers, train_genres, train_doc_keys, _ = \\\n",
    "    parse_dataset(\"data/raw/ontonotes/conll-2012/v5/data/train/data/english/annotations/*/*/*/*gold_conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11401 2775\n"
     ]
    }
   ],
   "source": [
    "valid_docids = []\n",
    "for docid in range(len(train_clusters)):\n",
    "    for sentid in range(len(train_clusters[docid])):\n",
    "        if set(train_clusters[docid][sentid]).__len__() > 1:\n",
    "#             print(docid, sentid, train_clusters[docid][sentid])\n",
    "            valid_docids.append(docid)\n",
    "            \n",
    "print(len(train_clusters), len(set(valid_docids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [['They', 'also', 'pay', 'out', 'salary', 'specially', 'providing', 'for', 'a', 'large', 'gang', 'of', 'female', 'duty', 'officers', ',', 'who', 'watch', 'the', 'screens', 'all', 'day', 'monitoring', 'intersections', ',', 'and', 'not', 'a', 'few', 'minutes', 'go', 'by', 'without', 'a', 'negative', 'report', '.'], ['That', 'female', 'voice', 'is', 'pure', 'misery', '.'], ['I', 'am', 'No.', 'xx', 'duty', 'officer', ',', 'according', 'to', 'the', 'screen', 'at', 'the', 'moment', ',', 'such', 'and', 'such', 'an', 'intersection', 'is', 'badly', 'backed', 'up', 'with', 'traffic', '.'], ['Please', ',', 'drivers', 'out', 'there', 'who', 'are', 'able', 'to', 'do', 'so', ',', 'make', 'an', 'attempt', 'to', 'bypass', '.'], ['It', \"'s\", 'bad', 'enough', 'as', 'it', 'is', 'churning', 'out', 'all', 'these', 'negative', 'reports', ',', 'but', 'then', 'commanding', 'others', 'to', 'listen', 'to', 'your', 'orders', ',', 'are', \"n't\", 'you', 'being', 'even', 'more', 'reactionary', '?'], ['And', 'even', 'that', \"'s\", 'not', 'enough', ','], ['every', 'day', 'they', 'work', 'on', 'pedestrians', 'to', 'ring', 'in', 'and', 'report', ',', 'such', 'a', 'place', 'is', 'blocked', 'up', 'again', ';', 'and', 'then', 'they', 'interrogate', 'them', ',', 'what', \"'s\", 'the', 'reason', 'for', 'the', 'jam', ',', 'how', 'long', 'has', 'it', 'been', 'jammed', 'for', ',', 'has', 'there', 'been', 'an', 'accident', ',', 'are', 'there', 'any', 'casualties', ',', 'are', 'there', 'any', '......', '.'], ['All', 'in', 'all', ',', 'not', 'a', 'single', 'nice', 'thing', 'to', 'say', '.'], ['Just', 'in', 'the', 'end', 'insincerely', 'coming', 'up', 'with', 'a', 'couple', 'of', 'words', 'of', 'warning', ',', 'drive', 'well', '.'], ['Is', \"n't\", 'that', 'crap', '?'], ['A', 'perfectly', 'good', 'mood', 'early', 'in', 'the', 'morning', 'totally', 'ruined', 'by', 'you', '.'], ['Misled', 'by', 'you', 'into', 'this', 'state', ',', 'how', 'could', 'I', 'not', 'get', 'into', 'an', 'accident', '?'], ['Even', 'more', 'impressive', ',', 'they', 'use', 'a', 'few', 'bad', 'boys', ',', 'one', 'called', 'Dong', 'Sheng', ',', 'one', 'called', 'Zhi', 'Yong', '.'], ['They', 'are', 'supposedly', 'teachers', '.'], ['Every', 'day', 'for', 'hours', 'on', 'air', ',', 'straight', 'out', 'insulting', 'and', 'abusing', '.'], ['If', 'you', 'are', 'abusing', 'ordinary', 'people', ',', 'then', 'that', \"'s\", 'one', 'thing', ',', 'but', 'it', \"'s\", 'always', 'abusing', 'the', 'authorities', ',', 'some', 'official', 'is', \"n't\", 'sympathetic', ',', 'does', \"n't\", 'accomplish', 'anything', ',', 'does', \"n't\", 'do', 'his', 'job', ',', 'basically', 'it', \"'s\", 'serious', 'defamation', '.']]\n",
    "clusters = [[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = [15, (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17, 17), (15, 17, 17), (15, 17, 17, 17z), (15, 17, 17), (15, 17, 17), (15, 17), (15, 17, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), (15, 17), -1, -1, 15, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, (14, 1), 14, 14, 14, 14, 14, 14, 14, (14, 1), 14, 14, 14, 14, 14, (14, 14), 14, 14, 14, 14, 14, 14, (14, 14), 14, 14, -1, -1, -1, -1, -1, -1, 14, 14, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = []\n",
    "open_spans= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for span, c in cluster:\n",
    "    \n",
    "    if c == -1:\n",
    "        continue\n",
    "    \n",
    "    if type(c) is int:\n",
    "        # There is only one annotation\n",
    "        if c in op.keys():\n",
    "            # We've seen this cluster before\n",
    "            # Check if this is an ongoing annotation or a new one\n",
    "            if c in [x[0] for x in open_spans]:\n",
    "                \n",
    "        else:\n",
    "            # Its a new cluster\n",
    "            op[c] = [(span,)]\n",
    "            open_spans.append((c, span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = {15, 17}\n",
    "expected = {12, 17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({12}, {15})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins = expected.intersection(cluster)\n",
    "expected - ins, cluster-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "for val in expected - ins:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "def subtraction(a: List[int], b: List[int]) -> List[int]:\n",
    "    \"\"\" Subtraction of one list from another (with duplicates) \"\"\"\n",
    "    if not b: \n",
    "        return deepcopy(a)\n",
    "    \n",
    "    b_ = deepcopy(b)\n",
    "    res = []\n",
    "    for ele in a:\n",
    "        if ele in b_:\n",
    "            b_.pop(b_.index(ele))  \n",
    "        else:\n",
    "            res.append(ele)\n",
    "        print(res, ele, b_)\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 [2, 3, 4, 5, 5, 5, 'potato']\n",
      "[1] 2 [3, 4, 5, 5, 5, 'potato']\n",
      "[1, 2] 2 [3, 4, 5, 5, 5, 'potato']\n",
      "[1, 2] 3 [4, 5, 5, 5, 'potato']\n",
      "[1, 2] 5 [4, 5, 5, 'potato']\n",
      "[1, 2] 5 [4, 5, 'potato']\n",
      "[1, 2] 5 [4, 'potato']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,2,3,5,5, 5]\n",
    "b = [2, 3,4,5,5,5, 'potato']\n",
    "\n",
    "subtraction(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_all(pattern: str, data: str):\n",
    "    \"\"\"Yields all the positions of\n",
    "    the pattern p in the string s.\"\"\"\n",
    "    return [a.start() for a in list(re.finditer(pattern, data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppattern = r'(\\([a-zA-Z]*)|\\)'\n",
    "pattern = r\"\\([a-zA-Z]*|\\)\"\n",
    "strings = \\\n",
    "'''(POOP\n",
    "*\n",
    "(A(B(C)(D))\n",
    ")\n",
    "*)'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(POOP', '*', '(A(B(C)(D))', ')', '*)']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(POOP\n",
      "\n",
      "(POOP\n",
      "---------\n",
      "*\n",
      "\n",
      "---------\n",
      "(A(B(C)(D))\n",
      "\n",
      "(A\n",
      "(B\n",
      "(C\n",
      ")\n",
      "(D\n",
      ")\n",
      ")\n",
      "---------\n",
      ")\n",
      "\n",
      ")\n",
      "---------\n",
      "*)\n",
      "\n",
      ")\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for string in strings:\n",
    "    print(string)\n",
    "    print()\n",
    "    for match in re.findall(pattern, string):\n",
    "        print(match)\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.__class__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
