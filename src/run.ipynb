{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing paths from /home/priyansh/Dev/research/coref/mtl/src\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import click\n",
    "import torch\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import List, Callable, Dict, Iterable\n",
    "\n",
    "# Local imports\n",
    "try:\n",
    "    import _pathfix\n",
    "except ImportError:\n",
    "    from . import _pathfix\n",
    "from loops import training_loop\n",
    "from config import LOCATIONS as LOC, CONFIG\n",
    "from models.multitask import BasicMTL\n",
    "from dataiter import MultiTaskDataIter\n",
    "# from eval import ner_all, ner_only_annotated, ner_span_recog_recall, ner_span_recog_precision, \\\n",
    "#     pruner_p, pruner_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15G        6,1G        2,5G        934M        6,7G        7,9G\r\n",
      "Swap:          979M          0B        979M\r\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_optimizer(model, optimizer_class: Callable, lr: float, freeze_encoder: bool):\n",
    "    if freeze_encoder:\n",
    "        return optimizer_class(\n",
    "            [param for name, param in model.named_parameters() if not name.startswith(\"encoder\")],\n",
    "            lr=lr\n",
    "        )\n",
    "    else:\n",
    "        return optimizer_class(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def get_pretrained_dirs(nm: str):\n",
    "    \"\"\"Check if the given nm is stored locally. If so, load that. Else, pass it on as is.\"\"\"\n",
    "    plausible_parent_dir: Path = LOC.root / \"models\" / \"huggingface\" / nm\n",
    "\n",
    "    if (\n",
    "            (plausible_parent_dir / \"config\").exists()\n",
    "            and (plausible_parent_dir / \"tokenizer\").exists()\n",
    "            and (plausible_parent_dir / \"encoder\").exists()\n",
    "    ):\n",
    "        return (\n",
    "            str(plausible_parent_dir / \"config\"),\n",
    "            str(plausible_parent_dir / \"tokenizer\"),\n",
    "            str(plausible_parent_dir / \"encoder\"),\n",
    "        )\n",
    "    else:\n",
    "        return nm, nm, nm\n",
    "\n",
    "\n",
    "def compute_metrics(metrics: Dict[str, Callable], logits, labels) -> Dict[str, float]:\n",
    "    return {metric_nm: metric_fn(logits=logits, labels=labels).item() for metric_nm, metric_fn in metrics.items()}\n",
    "\n",
    "\n",
    "def aggregate_metrics(inter_epoch: dict, intra_epoch: dict):\n",
    "    for task_nm in inter_epoch.keys():\n",
    "        for metric_nm, metric_list in intra_epoch[task_nm].items():\n",
    "            inter_epoch[task_nm][metric_nm].append(np.mean(metric_list))\n",
    "    return inter_epoch\n",
    "\n",
    "\n",
    "def simplest_loop(\n",
    "        epochs: int,\n",
    "        tasks: Iterable[str],\n",
    "        opt: torch.optim,\n",
    "        train_fn: Callable,\n",
    "        predict_fn: Callable,\n",
    "        trn_dl: Callable,\n",
    "        dev_dl: Callable,\n",
    "        eval_fns: dict,\n",
    ") -> (list, list, list):\n",
    "    train_loss = {task_nm: [] for task_nm in tasks}\n",
    "    train_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "    valid_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "\n",
    "    # Make data\n",
    "    trn_ds = trn_dl()\n",
    "    dev_ds = dev_dl()\n",
    "\n",
    "    # Epoch level\n",
    "    for e in range(epochs):\n",
    "\n",
    "        per_epoch_loss = {task_nm: [] for task_nm in tasks}\n",
    "        per_epoch_tr_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "        per_epoch_vl_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "\n",
    "        # Train\n",
    "        with Timer() as timer:\n",
    "\n",
    "            # Train Loop\n",
    "            for instance in tqdm(trn_ds):\n",
    "\n",
    "                # Reset the gradients.\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                outputs = train_fn(**instance)\n",
    "\n",
    "                \"\"\"\n",
    "                    Depending on instance.tasks list, do the following:\n",
    "                        - task specific loss (added to losses)\n",
    "                        - task specific metrics (added to metrics)\n",
    "                \"\"\"\n",
    "                for task_nm in instance['tasks']:\n",
    "                    loss = outputs[\"loss\"][task_nm]\n",
    "                    per_epoch_loss[task_nm].append(loss.item())\n",
    "\n",
    "                    # TODO: add other metrics here\n",
    "                    instance_metrics = compute_metrics(eval_fns[task_nm],\n",
    "                                                       logits=outputs[task_nm][\"logits\"],\n",
    "                                                       labels=outputs[task_nm][\"labels\"])\n",
    "                    for metric_nm, metric_vl in instance_metrics.items():\n",
    "                        per_epoch_tr_metrics[task_nm][metric_nm].append(metric_vl)\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # Val\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for instance in tqdm(dev_ds):\n",
    "                    outputs = predict_fn(**instance)\n",
    "\n",
    "                    for task_nm in instance[\"tasks\"]:\n",
    "                        logits = outputs[task_nm][\"logits\"]\n",
    "                        # TODO: make the label puller task specific somehow\n",
    "                        labels = instance[\"ner\"][\"gold_labels\"]\n",
    "\n",
    "                        instance_metrics = compute_metrics(eval_fns[task_nm], logits=logits, labels=labels)\n",
    "                        for metric_nm, metric_vl in instance_metrics.items():\n",
    "                            per_epoch_vl_metrics[task_nm][metric_nm].append(metric_vl)\n",
    "\n",
    "        # Bookkeep\n",
    "        for task_nm in tasks:\n",
    "            train_loss[task_nm].append(np.mean(per_epoch_loss[task_nm]))\n",
    "            train_metrics = aggregate_metrics(train_metrics, per_epoch_tr_metrics)\n",
    "            valid_metrics = aggregate_metrics(valid_metrics, per_epoch_vl_metrics)\n",
    "\n",
    "        print(f\"\\nEpoch: {e:3d}\" +\n",
    "              ''.join([f\" | {task_nm} Loss: {float(np.mean(per_epoch_loss[task_nm])):.5f}\" +\n",
    "                       ''.join([f\" | {task_nm} Tr_{metric_nm}: {float(metric_vls[-1]):.3f}\"\n",
    "                                for metric_nm, metric_vls in train_metrics[task_nm].items()]) +\n",
    "                       ''.join([f\" | {task_nm} Vl_{metric_nm}: {float(metric_vls[-1]):.3f}\"\n",
    "                                for metric_nm, metric_vls in valid_metrics[task_nm].items()])\n",
    "                       # f\" | {task_nm} Tr_c: {float(np.mean(per_epoch_tr_acc[task_nm])):.5f}\" +\n",
    "                       # f\" | {task_nm} Vl_c: {float(np.mean(per_epoch_vl_acc[task_nm])):.5f}\"\n",
    "                       for task_nm in tasks]))\n",
    "\n",
    "    return train_metrics, valid_metrics, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make MTL A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset: str = 'ontonotes'\n",
    "epochs: int = 10\n",
    "encoder: str = \"bert-base-uncased\"\n",
    "tasks: List[str] = ('coref', 'ner', 'pruner')\n",
    "device: str = \"cpu\"\n",
    "trim: bool = True\n",
    "train_encoder: bool = False,\n",
    "ner_unweighted: bool = False\n",
    "filter_candidates_pos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 318 instances from ../data/parsed/ontonotes/development/MultiTaskDatasetDump_coref_ner_pruner.pkl.\n",
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"binary_hdim\": 2000,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"coref_dropout\": 0.3,\n",
      "  \"device\": \"cpu\",\n",
      "  \"epochs\": 10,\n",
      "  \"filter_candidates_pos_threshold\": 2000,\n",
      "  \"freeze_encoder\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_span_width\": 5,\n",
      "  \"max_top_antecedents\": 50,\n",
      "  \"metadata_feature_size\": 20,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"ner_class_weights\": [\n",
      "    0.05330982413982514,\n",
      "    24.68555023923445,\n",
      "    30.717313646106216,\n",
      "    51.42822966507177,\n",
      "    20.887773279352228,\n",
      "    43.23902111967818,\n",
      "    101.32128829536528,\n",
      "    1697.1315789473683,\n",
      "    38.79157894736842,\n",
      "    174.06477732793522,\n",
      "    678.8526315789474,\n",
      "    135.77052631578948,\n",
      "    141.42763157894737,\n",
      "    102.85645933014354,\n",
      "    678.8526315789474,\n",
      "    1697.1315789473683,\n",
      "    183.47368421052633,\n",
      "    424.2828947368421,\n",
      "    1357.7052631578947\n",
      "  ],\n",
      "  \"ner_ignore_weights\": false,\n",
      "  \"ner_n_classes\": 19,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"pruner_class_weights\": [\n",
      "    0.5103185781885514,\n",
      "    24.728144171779142\n",
      "  ],\n",
      "  \"top_span_ratio\": 0.4,\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"trim\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unary_hdim\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": \"../models/huggingface/bert-base-uncased/config\"\n",
      "}\n",
      "\n",
      "Training commences!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyansh/Dev/research/coref/mtl/src/dataiter.py:97: UserWarning: The dataset has been trimmed to only 50 instances. This is NOT a legit experiment any more!\n",
      "  warnings.warn(\"The dataset has been trimmed to only 50 instances. This is NOT a legit experiment any more!\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_config, dir_tokenizer, dir_encoder = get_pretrained_dirs(encoder)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(dir_tokenizer)\n",
    "config = transformers.BertConfig(dir_config)\n",
    "config.max_span_width = 5\n",
    "config.coref_dropout = 0.3\n",
    "config.metadata_feature_size = 20\n",
    "config.unary_hdim = 1000\n",
    "config.binary_hdim = 2000\n",
    "config.top_span_ratio = 0.4\n",
    "config.max_top_antecedents = 50\n",
    "config.device = device\n",
    "config.epochs = epochs\n",
    "config.trim = trim\n",
    "config.freeze_encoder = not train_encoder\n",
    "config.ner_ignore_weights = ner_unweighted\n",
    "config.filter_candidates_pos_threshold = CONFIG['filter_candidates_pos_threshold'] \\\n",
    "    if filter_candidates_pos else -1\n",
    "\n",
    "\n",
    "if 'ner' in tasks or 'pruner' in tasks:\n",
    "    # Need to figure out the number of classes. Load a DL. Get the number. Delete the DL.\n",
    "    temp_ds = MultiTaskDataIter(\n",
    "        src=dataset,\n",
    "        config=config,\n",
    "        tasks=tasks,\n",
    "        split=\"development\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    if 'ner' in tasks:\n",
    "        config.ner_n_classes = deepcopy(temp_ds.ner_tag_dict.__len__())\n",
    "        config.ner_class_weights = temp_ds.estimate_class_weights('ner')\n",
    "    else:\n",
    "        config.ner_n_classes = 1\n",
    "        config.ner_class_weights = [1.0, ]\n",
    "    if 'pruner' in tasks:\n",
    "        config.pruner_class_weights = temp_ds.estimate_class_weights('pruner')\n",
    "    del temp_ds\n",
    "else:\n",
    "    config.ner_n_classes = 1\n",
    "    config.ner_class_weights = [1.0, ]\n",
    "\n",
    "# # Make the model\n",
    "# model = BasicMTL(dir_encoder, config=config)\n",
    "\n",
    "# Load the data\n",
    "train_ds = partial(\n",
    "    MultiTaskDataIter,\n",
    "    src=dataset,\n",
    "    config=config,\n",
    "    tasks=tasks,\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "# valid_ds = partial(\n",
    "#     MultiTaskDataIter,\n",
    "#     src=dataset,\n",
    "#     config=config,\n",
    "#     tasks=tasks,\n",
    "#     split=\"development\",\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# Make the optimizer\n",
    "# opt = make_optimizer(model=model, optimizer_class=torch.optim.SGD, lr=0.005, freeze_encoder=config.freeze_encoder)\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Make the evaluation suite (may compute multiple metrics corresponding to the tasks)\n",
    "# eval_fns: Dict[str, Dict[str, Callable]] = {\n",
    "#     'ner': {'acc': ner_all,\n",
    "#             'acc_l': ner_only_annotated,\n",
    "#             'span_p': ner_span_recog_precision,\n",
    "#             'span_r': ner_span_recog_recall},\n",
    "#     'coref': {\n",
    "\n",
    "#     },\n",
    "#     'pruner': {'p': pruner_p,\n",
    "#                'r': pruner_r}\n",
    "# }\n",
    "\n",
    "print(config)\n",
    "print(\"Training commences!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "onto_train_di = train_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make MTL B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 346 instances from ../data/parsed/scierc/train/MultiTaskDatasetDump_ner.pkl.\n"
     ]
    }
   ],
   "source": [
    "dataset: str = 'scierc'\n",
    "epochs: int = 10\n",
    "encoder: str = \"bert-base-uncased\"\n",
    "tasks: List[str] = ('ner',)\n",
    "device: str = \"cpu\"\n",
    "trim: bool = True\n",
    "train_encoder: bool = False\n",
    "ner_unweighted: bool = False\n",
    "filter_candidates_pos = True\n",
    "\n",
    "\n",
    "dir_config, dir_tokenizer, dir_encoder = get_pretrained_dirs(encoder)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(dir_tokenizer)\n",
    "config = transformers.BertConfig(dir_config)\n",
    "config.max_span_width = 5\n",
    "config.coref_dropout = 0.3\n",
    "config.metadata_feature_size = 20\n",
    "config.unary_hdim = 1000\n",
    "config.binary_hdim = 2000\n",
    "config.top_span_ratio = 0.4\n",
    "config.max_top_antecedents = 50\n",
    "config.device = device\n",
    "config.epochs = epochs\n",
    "config.trim = trim\n",
    "config.freeze_encoder = not train_encoder\n",
    "config.ner_ignore_weights = ner_unweighted\n",
    "config.filter_candidates_pos_threshold = CONFIG['filter_candidates_pos_threshold'] \\\n",
    "    if filter_candidates_pos else -1\n",
    "\n",
    "\n",
    "# if 'ner' in tasks or 'pruner' in tasks:\n",
    "if False:\n",
    "    # Need to figure out the number of classes. Load a DL. Get the number. Delete the DL.\n",
    "    temp_ds = MultiTaskDataIter(\n",
    "        src=dataset,\n",
    "        config=config,\n",
    "        tasks=tasks,\n",
    "        split=\"dev\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    if 'ner' in tasks:\n",
    "        config.ner_n_classes = deepcopy(temp_ds.ner_tag_dict.__len__())\n",
    "        config.ner_class_weights = temp_ds.estimate_class_weights('ner')\n",
    "    else:\n",
    "        config.ner_n_classes = 1\n",
    "        config.ner_class_weights = [1.0, ]\n",
    "    if 'pruner' in tasks:\n",
    "        config.pruner_class_weights = temp_ds.estimate_class_weights('pruner')\n",
    "    del temp_ds\n",
    "else:\n",
    "    config.ner_n_classes = 1\n",
    "    config.ner_class_weights = [1.0, ]\n",
    "\n",
    "# # Make the model\n",
    "# model = BasicMTL(dir_encoder, config=config)\n",
    "\n",
    "# Load the data\n",
    "train_ds_b = partial(\n",
    "    MultiTaskDataIter,\n",
    "    src=dataset,\n",
    "    config=config,\n",
    "    tasks=tasks,\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sci_train_ds = train_ds_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataiter import DataIterCombiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 2455 instances from ../data/parsed/ontonotes/train/MultiTaskDatasetDump_coref_ner_pruner.pkl.\n",
      "Pulled 346 instances from ../data/parsed/scierc/train/MultiTaskDatasetDump_ner.pkl.\n"
     ]
    }
   ],
   "source": [
    "dc = DataIterCombiner([train_ds, train_ds_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (1, 1), 1: (1, 2), 2: (1, 3)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': ['ner'],\n",
       " 'input_ids': tensor([[ 1999,  2023,  3259,  1010,  1037,  3117,  4118,  2000,  4553,  1996,\n",
       "          23807,  4874,  3252,  2005, 15873,  5107,  9651,  2003,  3818,  1012,\n",
       "           1996,  3937, 11213,  2003,  2008,  1996, 16381,  3550,  4874,  2110,\n",
       "           3658,  2006,  1037,  2659,  8789, 19726,  1998,  2064,  2022,  4342,\n",
       "           2013,  2731,  2951,  1012,  2241,  2006,  2023, 11213,  1010, 15847,\n",
       "           2057,  5173,  1996,  8789,  3012,  7312,  1998,  4304, 24155,  9896,\n",
       "           2005,  4895,  6342,  4842, 11365,  2098,  4083,  1997,  4874, 23807,\n",
       "           6630,  1010,  1996,  4663,  2512,  1011, 11841,  2112,  1997,  4874,\n",
       "           2110, 13416,  2130,  2000,  1016,  9646,  1012, 16378,  1996,  8790,\n",
       "           2389,  2944,  2003,  5173,  1998,  4738,  2241,  2006,  2023, 23807,\n",
       "           6630,  1012,  2353,  2135,  1996,  4342, 23807,  4874,  3252,  2003,\n",
       "           6377,  2046,  1037, 10811,  1011, 11307,  2806, 27080,  1012,  2057,\n",
       "           2097,  2265,  2008,  2023, 23807,  4874,  6630,  2038,  2070,  5875,\n",
       "           5144,  1998,  2241,  2006,  2029,  1996,  4397,  5173,  8790,  2389,\n",
       "           2944,  3084, 10811,  1011, 11307,  2806, 27080,  2062, 15873,  1998,\n",
       "          10539,  1012,  7885,  2265,  2008,  1996,  4342, 27080, 10438,  2172,\n",
       "           2488,  2084,  4493, 27080,  2015,  2006,  1996,  9651,  1997,  3375,\n",
       "           2512,  1011, 11841, 15323,  2107,  2004,  3869, 12814,  2007,  2969,\n",
       "           1011,  1051,  9468, 24117,  1998,  2312,  6970,  1011,  4853,  5423,\n",
       "           4367,  1012,  1996,  3818,  4118,  2036,  2038,  1996,  4022,  2000,\n",
       "           9611,  2060,  2828,  1997,  9651,  3471,  1012,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'word_map': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  26,\n",
       "          27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "          41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  52,  53,\n",
       "          54,  55,  56,  57,  58,  59,  59,  59,  59,  59,  60,  61,  62,  63,\n",
       "          64,  65,  66,  67,  68,  68,  68,  69,  70,  71,  72,  73,  74,  75,\n",
       "          76,  77,  78,  79,  80,  81,  81,  82,  83,  84,  85,  86,  87,  88,\n",
       "          89,  90,  91,  92,  93,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
       "         102, 103, 103, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
       "         114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 126,\n",
       "         127, 128, 129, 129, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "         139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 148, 149, 150, 151,\n",
       "         152, 153, 154, 154, 154, 155, 156, 157, 158, 159, 160, 161, 161, 161,\n",
       "         161, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171,\n",
       "         172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]),\n",
       " 'sentence_map': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]),\n",
       " 'n_words': 183,\n",
       " 'n_subwords': 207,\n",
       " 'candidate_starts': tensor([  0,   0,   0,   0,   0,   1,   1,   1,   1,   1,   2,   2,   2,   2,\n",
       "           2,   3,   3,   3,   3,   3,   4,   4,   4,   4,   4,   5,   5,   5,\n",
       "           5,   5,   6,   6,   6,   6,   6,   7,   7,   7,   7,   7,   8,   8,\n",
       "           8,   8,   8,   9,   9,   9,   9,   9,  10,  10,  10,  10,  10,  11,\n",
       "          11,  11,  11,  11,  12,  12,  12,  12,  12,  13,  13,  13,  13,  13,\n",
       "          14,  14,  14,  14,  14,  15,  15,  15,  15,  15,  16,  16,  16,  16,\n",
       "          17,  17,  17,  18,  18,  19,  20,  20,  20,  20,  20,  21,  21,  21,\n",
       "          21,  21,  22,  22,  22,  22,  22,  23,  23,  23,  23,  23,  24,  24,\n",
       "          24,  24,  24,  25,  25,  25,  25,  25,  26,  26,  26,  26,  26,  27,\n",
       "          27,  27,  27,  27,  28,  28,  28,  28,  28,  29,  29,  29,  29,  29,\n",
       "          30,  30,  30,  30,  30,  31,  31,  31,  31,  31,  32,  32,  32,  32,\n",
       "          32,  33,  33,  33,  33,  33,  34,  34,  34,  34,  34,  35,  35,  35,\n",
       "          35,  35,  36,  36,  36,  36,  36,  37,  37,  37,  37,  37,  38,  38,\n",
       "          38,  38,  38,  39,  39,  39,  39,  39,  40,  40,  40,  40,  41,  41,\n",
       "          41,  42,  42,  43,  44,  44,  44,  44,  44,  45,  45,  45,  45,  45,\n",
       "          46,  46,  46,  46,  46,  47,  47,  47,  47,  47,  48,  48,  48,  48,\n",
       "          48,  49,  49,  49,  49,  49,  50,  50,  50,  50,  50,  51,  51,  51,\n",
       "          51,  51,  52,  52,  52,  52,  52,  53,  53,  53,  53,  53,  54,  54,\n",
       "          54,  54,  54,  55,  55,  55,  55,  55,  56,  56,  56,  56,  56,  57,\n",
       "          57,  57,  57,  57,  58,  58,  58,  58,  58,  59,  59,  59,  59,  59,\n",
       "          60,  60,  60,  60,  60,  61,  61,  61,  61,  61,  62,  62,  62,  62,\n",
       "          62,  63,  63,  63,  63,  63,  64,  64,  64,  64,  64,  65,  65,  65,\n",
       "          65,  65,  66,  66,  66,  66,  66,  67,  67,  67,  67,  67,  68,  68,\n",
       "          68,  68,  68,  69,  69,  69,  69,  69,  70,  70,  70,  70,  70,  71,\n",
       "          71,  71,  71,  71,  72,  72,  72,  72,  72,  73,  73,  73,  73,  73,\n",
       "          74,  74,  74,  74,  74,  75,  75,  75,  75,  75,  76,  76,  76,  76,\n",
       "          76,  77,  77,  77,  77,  77,  78,  78,  78,  78,  78,  79,  79,  79,\n",
       "          79,  79,  80,  80,  80,  80,  80,  81,  81,  81,  81,  81,  82,  82,\n",
       "          82,  82,  82,  83,  83,  83,  83,  84,  84,  84,  85,  85,  86,  87,\n",
       "          87,  87,  87,  87,  88,  88,  88,  88,  88,  89,  89,  89,  89,  89,\n",
       "          90,  90,  90,  90,  90,  91,  91,  91,  91,  91,  92,  92,  92,  92,\n",
       "          92,  93,  93,  93,  93,  93,  94,  94,  94,  94,  94,  95,  95,  95,\n",
       "          95,  95,  96,  96,  96,  96,  96,  97,  97,  97,  97,  97,  98,  98,\n",
       "          98,  98,  99,  99,  99, 100, 100, 101, 102, 102, 102, 102, 102, 103,\n",
       "         103, 103, 103, 103, 104, 104, 104, 104, 104, 105, 105, 105, 105, 105,\n",
       "         106, 106, 106, 106, 106, 107, 107, 107, 107, 107, 108, 108, 108, 108,\n",
       "         108, 109, 109, 109, 109, 109, 110, 110, 110, 110, 110, 111, 111, 111,\n",
       "         111, 111, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 114, 114,\n",
       "         114, 114, 114, 115, 115, 115, 115, 116, 116, 116, 117, 117, 118, 119,\n",
       "         119, 119, 119, 119, 120, 120, 120, 120, 120, 121, 121, 121, 121, 121,\n",
       "         122, 122, 122, 122, 122, 123, 123, 123, 123, 123, 124, 124, 124, 124,\n",
       "         124, 125, 125, 125, 125, 125, 126, 126, 126, 126, 126, 127, 127, 127,\n",
       "         127, 127, 128, 128, 128, 128, 128, 129, 129, 129, 129, 129, 130, 130,\n",
       "         130, 130, 130, 131, 131, 131, 131, 131, 132, 132, 132, 132, 132, 133,\n",
       "         133, 133, 133, 133, 134, 134, 134, 134, 134, 135, 135, 135, 135, 135,\n",
       "         136, 136, 136, 136, 136, 137, 137, 137, 137, 137, 138, 138, 138, 138,\n",
       "         138, 139, 139, 139, 139, 139, 140, 140, 140, 140, 140, 141, 141, 141,\n",
       "         141, 141, 142, 142, 142, 142, 142, 143, 143, 143, 143, 143, 144, 144,\n",
       "         144, 144, 144, 145, 145, 145, 145, 145, 146, 146, 146, 146, 146, 147,\n",
       "         147, 147, 147, 147, 148, 148, 148, 148, 149, 149, 149, 150, 150, 151,\n",
       "         152, 152, 152, 152, 152, 153, 153, 153, 153, 153, 154, 154, 154, 154,\n",
       "         154, 155, 155, 155, 155, 155, 156, 156, 156, 156, 156, 157, 157, 157,\n",
       "         157, 157, 158, 158, 158, 158, 158, 159, 159, 159, 159, 159, 160, 160,\n",
       "         160, 160, 160, 161, 161, 161, 161, 161, 162, 162, 162, 162, 162, 163,\n",
       "         163, 163, 163, 163, 164, 164, 164, 164, 164, 165, 165, 165, 165, 165,\n",
       "         166, 166, 166, 166, 166, 167, 167, 167, 167, 167, 168, 168, 168, 168,\n",
       "         168, 169, 169, 169, 169, 169, 170, 170, 170, 170, 170, 171, 171, 171,\n",
       "         171, 171, 172, 172, 172, 172, 172, 173, 173, 173, 173, 173, 174, 174,\n",
       "         174, 174, 174, 175, 175, 175, 175, 175, 176, 176, 176, 176, 176, 177,\n",
       "         177, 177, 177, 177, 178, 178, 178, 178, 178, 179, 179, 179, 179, 179,\n",
       "         180, 180, 180, 180, 180, 181, 181, 181, 181, 181, 182, 182, 182, 182,\n",
       "         182, 183, 183, 183, 183, 183, 184, 184, 184, 184, 184, 185, 185, 185,\n",
       "         185, 185, 186, 186, 186, 186, 186, 187, 187, 187, 187, 187, 188, 188,\n",
       "         188, 188, 189, 189, 189, 190, 190, 191, 192, 192, 192, 192, 192, 193,\n",
       "         193, 193, 193, 193, 194, 194, 194, 194, 194, 195, 195, 195, 195, 195,\n",
       "         196, 196, 196, 196, 196, 197, 197, 197, 197, 197, 198, 198, 198, 198,\n",
       "         198, 199, 199, 199, 199, 199, 200, 200, 200, 200, 200, 201, 201, 201,\n",
       "         201, 201, 202, 202, 202, 202, 202, 203, 203, 203, 203, 204, 204, 204,\n",
       "         205, 205, 206]),\n",
       " 'candidate_ends': tensor([  0,   1,   2,   3,   4,   1,   2,   3,   4,   5,   2,   3,   4,   5,\n",
       "           6,   3,   4,   5,   6,   7,   4,   5,   6,   7,   8,   5,   6,   7,\n",
       "           8,   9,   6,   7,   8,   9,  10,   7,   8,   9,  10,  11,   8,   9,\n",
       "          10,  11,  12,   9,  10,  11,  12,  13,  10,  11,  12,  13,  14,  11,\n",
       "          12,  13,  14,  15,  12,  13,  14,  15,  16,  13,  14,  15,  16,  17,\n",
       "          14,  15,  16,  17,  18,  15,  16,  17,  18,  19,  16,  17,  18,  19,\n",
       "          17,  18,  19,  18,  19,  19,  20,  21,  22,  23,  24,  21,  22,  23,\n",
       "          24,  25,  22,  23,  24,  25,  26,  23,  24,  25,  26,  27,  24,  25,\n",
       "          26,  27,  28,  25,  26,  27,  28,  29,  26,  27,  28,  29,  30,  27,\n",
       "          28,  29,  30,  31,  28,  29,  30,  31,  32,  29,  30,  31,  32,  33,\n",
       "          30,  31,  32,  33,  34,  31,  32,  33,  34,  35,  32,  33,  34,  35,\n",
       "          36,  33,  34,  35,  36,  37,  34,  35,  36,  37,  38,  35,  36,  37,\n",
       "          38,  39,  36,  37,  38,  39,  40,  37,  38,  39,  40,  41,  38,  39,\n",
       "          40,  41,  42,  39,  40,  41,  42,  43,  40,  41,  42,  43,  41,  42,\n",
       "          43,  42,  43,  43,  44,  45,  46,  47,  48,  45,  46,  47,  48,  49,\n",
       "          46,  47,  48,  49,  50,  47,  48,  49,  50,  51,  48,  49,  50,  51,\n",
       "          52,  49,  50,  51,  52,  53,  50,  51,  52,  53,  54,  51,  52,  53,\n",
       "          54,  55,  52,  53,  54,  55,  56,  53,  54,  55,  56,  57,  54,  55,\n",
       "          56,  57,  58,  55,  56,  57,  58,  59,  56,  57,  58,  59,  60,  57,\n",
       "          58,  59,  60,  61,  58,  59,  60,  61,  62,  59,  60,  61,  62,  63,\n",
       "          60,  61,  62,  63,  64,  61,  62,  63,  64,  65,  62,  63,  64,  65,\n",
       "          66,  63,  64,  65,  66,  67,  64,  65,  66,  67,  68,  65,  66,  67,\n",
       "          68,  69,  66,  67,  68,  69,  70,  67,  68,  69,  70,  71,  68,  69,\n",
       "          70,  71,  72,  69,  70,  71,  72,  73,  70,  71,  72,  73,  74,  71,\n",
       "          72,  73,  74,  75,  72,  73,  74,  75,  76,  73,  74,  75,  76,  77,\n",
       "          74,  75,  76,  77,  78,  75,  76,  77,  78,  79,  76,  77,  78,  79,\n",
       "          80,  77,  78,  79,  80,  81,  78,  79,  80,  81,  82,  79,  80,  81,\n",
       "          82,  83,  80,  81,  82,  83,  84,  81,  82,  83,  84,  85,  82,  83,\n",
       "          84,  85,  86,  83,  84,  85,  86,  84,  85,  86,  85,  86,  86,  87,\n",
       "          88,  89,  90,  91,  88,  89,  90,  91,  92,  89,  90,  91,  92,  93,\n",
       "          90,  91,  92,  93,  94,  91,  92,  93,  94,  95,  92,  93,  94,  95,\n",
       "          96,  93,  94,  95,  96,  97,  94,  95,  96,  97,  98,  95,  96,  97,\n",
       "          98,  99,  96,  97,  98,  99, 100,  97,  98,  99, 100, 101,  98,  99,\n",
       "         100, 101,  99, 100, 101, 100, 101, 101, 102, 103, 104, 105, 106, 103,\n",
       "         104, 105, 106, 107, 104, 105, 106, 107, 108, 105, 106, 107, 108, 109,\n",
       "         106, 107, 108, 109, 110, 107, 108, 109, 110, 111, 108, 109, 110, 111,\n",
       "         112, 109, 110, 111, 112, 113, 110, 111, 112, 113, 114, 111, 112, 113,\n",
       "         114, 115, 112, 113, 114, 115, 116, 113, 114, 115, 116, 117, 114, 115,\n",
       "         116, 117, 118, 115, 116, 117, 118, 116, 117, 118, 117, 118, 118, 119,\n",
       "         120, 121, 122, 123, 120, 121, 122, 123, 124, 121, 122, 123, 124, 125,\n",
       "         122, 123, 124, 125, 126, 123, 124, 125, 126, 127, 124, 125, 126, 127,\n",
       "         128, 125, 126, 127, 128, 129, 126, 127, 128, 129, 130, 127, 128, 129,\n",
       "         130, 131, 128, 129, 130, 131, 132, 129, 130, 131, 132, 133, 130, 131,\n",
       "         132, 133, 134, 131, 132, 133, 134, 135, 132, 133, 134, 135, 136, 133,\n",
       "         134, 135, 136, 137, 134, 135, 136, 137, 138, 135, 136, 137, 138, 139,\n",
       "         136, 137, 138, 139, 140, 137, 138, 139, 140, 141, 138, 139, 140, 141,\n",
       "         142, 139, 140, 141, 142, 143, 140, 141, 142, 143, 144, 141, 142, 143,\n",
       "         144, 145, 142, 143, 144, 145, 146, 143, 144, 145, 146, 147, 144, 145,\n",
       "         146, 147, 148, 145, 146, 147, 148, 149, 146, 147, 148, 149, 150, 147,\n",
       "         148, 149, 150, 151, 148, 149, 150, 151, 149, 150, 151, 150, 151, 151,\n",
       "         152, 153, 154, 155, 156, 153, 154, 155, 156, 157, 154, 155, 156, 157,\n",
       "         158, 155, 156, 157, 158, 159, 156, 157, 158, 159, 160, 157, 158, 159,\n",
       "         160, 161, 158, 159, 160, 161, 162, 159, 160, 161, 162, 163, 160, 161,\n",
       "         162, 163, 164, 161, 162, 163, 164, 165, 162, 163, 164, 165, 166, 163,\n",
       "         164, 165, 166, 167, 164, 165, 166, 167, 168, 165, 166, 167, 168, 169,\n",
       "         166, 167, 168, 169, 170, 167, 168, 169, 170, 171, 168, 169, 170, 171,\n",
       "         172, 169, 170, 171, 172, 173, 170, 171, 172, 173, 174, 171, 172, 173,\n",
       "         174, 175, 172, 173, 174, 175, 176, 173, 174, 175, 176, 177, 174, 175,\n",
       "         176, 177, 178, 175, 176, 177, 178, 179, 176, 177, 178, 179, 180, 177,\n",
       "         178, 179, 180, 181, 178, 179, 180, 181, 182, 179, 180, 181, 182, 183,\n",
       "         180, 181, 182, 183, 184, 181, 182, 183, 184, 185, 182, 183, 184, 185,\n",
       "         186, 183, 184, 185, 186, 187, 184, 185, 186, 187, 188, 185, 186, 187,\n",
       "         188, 189, 186, 187, 188, 189, 190, 187, 188, 189, 190, 191, 188, 189,\n",
       "         190, 191, 189, 190, 191, 190, 191, 191, 192, 193, 194, 195, 196, 193,\n",
       "         194, 195, 196, 197, 194, 195, 196, 197, 198, 195, 196, 197, 198, 199,\n",
       "         196, 197, 198, 199, 200, 197, 198, 199, 200, 201, 198, 199, 200, 201,\n",
       "         202, 199, 200, 201, 202, 203, 200, 201, 202, 203, 204, 201, 202, 203,\n",
       "         204, 205, 202, 203, 204, 205, 206, 203, 204, 205, 206, 204, 205, 206,\n",
       "         205, 206, 206]),\n",
       " 'ner': {'gold_starts': tensor([  6,  10,  14,  26,  33,  53,  61,  68,  74,  89,  99, 106, 113, 124,\n",
       "          138, 142, 157, 163, 167, 169, 176, 179, 186, 194, 204]),\n",
       "  'gold_ends': tensor([  6,  12,  16,  29,  35,  59,  70,  70,  80,  91, 100, 108, 117, 126,\n",
       "          140, 146, 157, 164, 173, 173, 177, 183, 190, 194, 205]),\n",
       "  'gold_labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0])}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dc:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': ['ner'],\n",
       " 'input_ids': tensor([[10629, 16905,  3464,  1996,  2087,  6450,  2112,  1997, 16905,  1011,\n",
       "           2241,  8035, 11968,  7741,  1012,  2057,  3579,  2006,  2028,  3177,\n",
       "           1011,  2039,  5783,  1999,  1996,  2640,  1997, 16905, 13792,  1024,\n",
       "          24685,  1997, 24731,  1997,  4895,  5302,  4305, 10451,  4942, 27341,\n",
       "           1012,  2057, 16599,  1037,  4118,  1997, 26615,  2107,  1037,  2640,\n",
       "           2083,  1037,  4118,  1997,  3252,  1011,  6631,  2029, 26777,  8833,\n",
       "           1006,  1040,  1007,  8964,  2015,  2411,  3378,  2007,  3252,  1011,\n",
       "           6631,  1997, 19287,  2302,  2151,  2224,  1997, 17047, 24394, 20884,\n",
       "           2015,  1012,  1996,  3818,  5679, 11027,  2015, 21707, 24731,  2096,\n",
       "           8498,  1996, 17982,  1011, 15615,  5679,  1005,  1055,  3754,  2000,\n",
       "           4468,  2058, 24731,  1998,  2220, 24731,  4117,  2007,  2049,  3754,\n",
       "           2000,  5047, 23750,  5090,  2302,  9896,  2594, 13134,  1012,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'word_map': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  8,  8,  9, 10, 10, 11, 12, 13, 14,\n",
       "         15, 16, 16, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29,\n",
       "         29, 29, 30, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
       "         45, 45, 45, 46, 47, 48, 49, 50, 51, 52, 52, 53, 54, 55, 56, 56, 56, 57,\n",
       "         58, 59, 60, 61, 62, 63, 64, 65, 65, 66, 67, 68, 69, 70, 70, 71, 72, 73,\n",
       "         74, 75, 76, 76, 76, 77, 78, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88,\n",
       "         89, 90, 91, 92, 93, 94, 95, 96, 96, 97, 98]),\n",
       " 'sentence_map': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]),\n",
       " 'n_words': 99,\n",
       " 'n_subwords': 119,\n",
       " 'candidate_starts': tensor([  0,   0,   0,   0,   0,   1,   1,   1,   1,   1,   2,   2,   2,   2,\n",
       "           2,   3,   3,   3,   3,   3,   4,   4,   4,   4,   4,   5,   5,   5,\n",
       "           5,   5,   6,   6,   6,   6,   6,   7,   7,   7,   7,   7,   8,   8,\n",
       "           8,   8,   8,   9,   9,   9,   9,   9,  10,  10,  10,  10,  10,  11,\n",
       "          11,  11,  11,  12,  12,  12,  13,  13,  14,  15,  15,  15,  15,  15,\n",
       "          16,  16,  16,  16,  16,  17,  17,  17,  17,  17,  18,  18,  18,  18,\n",
       "          18,  19,  19,  19,  19,  19,  20,  20,  20,  20,  20,  21,  21,  21,\n",
       "          21,  21,  22,  22,  22,  22,  22,  23,  23,  23,  23,  23,  24,  24,\n",
       "          24,  24,  24,  25,  25,  25,  25,  25,  26,  26,  26,  26,  26,  27,\n",
       "          27,  27,  27,  27,  28,  28,  28,  28,  28,  29,  29,  29,  29,  29,\n",
       "          30,  30,  30,  30,  30,  31,  31,  31,  31,  31,  32,  32,  32,  32,\n",
       "          32,  33,  33,  33,  33,  33,  34,  34,  34,  34,  34,  35,  35,  35,\n",
       "          35,  35,  36,  36,  36,  36,  36,  37,  37,  37,  37,  38,  38,  38,\n",
       "          39,  39,  40,  41,  41,  41,  41,  41,  42,  42,  42,  42,  42,  43,\n",
       "          43,  43,  43,  43,  44,  44,  44,  44,  44,  45,  45,  45,  45,  45,\n",
       "          46,  46,  46,  46,  46,  47,  47,  47,  47,  47,  48,  48,  48,  48,\n",
       "          48,  49,  49,  49,  49,  49,  50,  50,  50,  50,  50,  51,  51,  51,\n",
       "          51,  51,  52,  52,  52,  52,  52,  53,  53,  53,  53,  53,  54,  54,\n",
       "          54,  54,  54,  55,  55,  55,  55,  55,  56,  56,  56,  56,  56,  57,\n",
       "          57,  57,  57,  57,  58,  58,  58,  58,  58,  59,  59,  59,  59,  59,\n",
       "          60,  60,  60,  60,  60,  61,  61,  61,  61,  61,  62,  62,  62,  62,\n",
       "          62,  63,  63,  63,  63,  63,  64,  64,  64,  64,  64,  65,  65,  65,\n",
       "          65,  65,  66,  66,  66,  66,  66,  67,  67,  67,  67,  67,  68,  68,\n",
       "          68,  68,  68,  69,  69,  69,  69,  69,  70,  70,  70,  70,  70,  71,\n",
       "          71,  71,  71,  71,  72,  72,  72,  72,  72,  73,  73,  73,  73,  73,\n",
       "          74,  74,  74,  74,  74,  75,  75,  75,  75,  75,  76,  76,  76,  76,\n",
       "          76,  77,  77,  77,  77,  77,  78,  78,  78,  78,  79,  79,  79,  80,\n",
       "          80,  81,  82,  82,  82,  82,  82,  83,  83,  83,  83,  83,  84,  84,\n",
       "          84,  84,  84,  85,  85,  85,  85,  85,  86,  86,  86,  86,  86,  87,\n",
       "          87,  87,  87,  87,  88,  88,  88,  88,  88,  89,  89,  89,  89,  89,\n",
       "          90,  90,  90,  90,  90,  91,  91,  91,  91,  91,  92,  92,  92,  92,\n",
       "          92,  93,  93,  93,  93,  93,  94,  94,  94,  94,  94,  95,  95,  95,\n",
       "          95,  95,  96,  96,  96,  96,  96,  97,  97,  97,  97,  97,  98,  98,\n",
       "          98,  98,  98,  99,  99,  99,  99,  99, 100, 100, 100, 100, 100, 101,\n",
       "         101, 101, 101, 101, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103,\n",
       "         104, 104, 104, 104, 104, 105, 105, 105, 105, 105, 106, 106, 106, 106,\n",
       "         106, 107, 107, 107, 107, 107, 108, 108, 108, 108, 108, 109, 109, 109,\n",
       "         109, 109, 110, 110, 110, 110, 110, 111, 111, 111, 111, 111, 112, 112,\n",
       "         112, 112, 112, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114, 115,\n",
       "         115, 115, 115, 116, 116, 116, 117, 117, 118]),\n",
       " 'candidate_ends': tensor([  0,   1,   2,   3,   4,   1,   2,   3,   4,   5,   2,   3,   4,   5,\n",
       "           6,   3,   4,   5,   6,   7,   4,   5,   6,   7,   8,   5,   6,   7,\n",
       "           8,   9,   6,   7,   8,   9,  10,   7,   8,   9,  10,  11,   8,   9,\n",
       "          10,  11,  12,   9,  10,  11,  12,  13,  10,  11,  12,  13,  14,  11,\n",
       "          12,  13,  14,  12,  13,  14,  13,  14,  14,  15,  16,  17,  18,  19,\n",
       "          16,  17,  18,  19,  20,  17,  18,  19,  20,  21,  18,  19,  20,  21,\n",
       "          22,  19,  20,  21,  22,  23,  20,  21,  22,  23,  24,  21,  22,  23,\n",
       "          24,  25,  22,  23,  24,  25,  26,  23,  24,  25,  26,  27,  24,  25,\n",
       "          26,  27,  28,  25,  26,  27,  28,  29,  26,  27,  28,  29,  30,  27,\n",
       "          28,  29,  30,  31,  28,  29,  30,  31,  32,  29,  30,  31,  32,  33,\n",
       "          30,  31,  32,  33,  34,  31,  32,  33,  34,  35,  32,  33,  34,  35,\n",
       "          36,  33,  34,  35,  36,  37,  34,  35,  36,  37,  38,  35,  36,  37,\n",
       "          38,  39,  36,  37,  38,  39,  40,  37,  38,  39,  40,  38,  39,  40,\n",
       "          39,  40,  40,  41,  42,  43,  44,  45,  42,  43,  44,  45,  46,  43,\n",
       "          44,  45,  46,  47,  44,  45,  46,  47,  48,  45,  46,  47,  48,  49,\n",
       "          46,  47,  48,  49,  50,  47,  48,  49,  50,  51,  48,  49,  50,  51,\n",
       "          52,  49,  50,  51,  52,  53,  50,  51,  52,  53,  54,  51,  52,  53,\n",
       "          54,  55,  52,  53,  54,  55,  56,  53,  54,  55,  56,  57,  54,  55,\n",
       "          56,  57,  58,  55,  56,  57,  58,  59,  56,  57,  58,  59,  60,  57,\n",
       "          58,  59,  60,  61,  58,  59,  60,  61,  62,  59,  60,  61,  62,  63,\n",
       "          60,  61,  62,  63,  64,  61,  62,  63,  64,  65,  62,  63,  64,  65,\n",
       "          66,  63,  64,  65,  66,  67,  64,  65,  66,  67,  68,  65,  66,  67,\n",
       "          68,  69,  66,  67,  68,  69,  70,  67,  68,  69,  70,  71,  68,  69,\n",
       "          70,  71,  72,  69,  70,  71,  72,  73,  70,  71,  72,  73,  74,  71,\n",
       "          72,  73,  74,  75,  72,  73,  74,  75,  76,  73,  74,  75,  76,  77,\n",
       "          74,  75,  76,  77,  78,  75,  76,  77,  78,  79,  76,  77,  78,  79,\n",
       "          80,  77,  78,  79,  80,  81,  78,  79,  80,  81,  79,  80,  81,  80,\n",
       "          81,  81,  82,  83,  84,  85,  86,  83,  84,  85,  86,  87,  84,  85,\n",
       "          86,  87,  88,  85,  86,  87,  88,  89,  86,  87,  88,  89,  90,  87,\n",
       "          88,  89,  90,  91,  88,  89,  90,  91,  92,  89,  90,  91,  92,  93,\n",
       "          90,  91,  92,  93,  94,  91,  92,  93,  94,  95,  92,  93,  94,  95,\n",
       "          96,  93,  94,  95,  96,  97,  94,  95,  96,  97,  98,  95,  96,  97,\n",
       "          98,  99,  96,  97,  98,  99, 100,  97,  98,  99, 100, 101,  98,  99,\n",
       "         100, 101, 102,  99, 100, 101, 102, 103, 100, 101, 102, 103, 104, 101,\n",
       "         102, 103, 104, 105, 102, 103, 104, 105, 106, 103, 104, 105, 106, 107,\n",
       "         104, 105, 106, 107, 108, 105, 106, 107, 108, 109, 106, 107, 108, 109,\n",
       "         110, 107, 108, 109, 110, 111, 108, 109, 110, 111, 112, 109, 110, 111,\n",
       "         112, 113, 110, 111, 112, 113, 114, 111, 112, 113, 114, 115, 112, 113,\n",
       "         114, 115, 116, 113, 114, 115, 116, 117, 114, 115, 116, 117, 118, 115,\n",
       "         116, 117, 118, 116, 117, 118, 117, 118, 118]),\n",
       " 'ner': {'gold_starts': tensor([  0,   8,  19,  27,  32,  34,  44,  54,  59,  68,  78,  84,  87,  92,\n",
       "          101, 104, 112]),\n",
       "  'gold_ends': tensor([  1,  13,  22,  28,  39,  39,  44,  56,  64,  72,  80,  84,  88,  98,\n",
       "          102, 105, 113]),\n",
       "  'gold_labels': tensor([0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0])}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sci_train_ds[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = onto_train_di.__len__()\n",
    "lb = sci_train_ds.__len__()\n",
    "\n",
    "la, lb, la+lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_ratio = [0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llen = la + lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointers = [int(x* llen / float(sum(sampling_ratio))) for x in sampling_ratio]\n",
    "pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointers = [2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_indices = []\n",
    "for i, dataset_specific_ratio in enumerate(pointers):\n",
    "    source_indices += [i]*dataset_specific_ratio\n",
    "    \n",
    "source_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(source_indices)\n",
    "\n",
    "source_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from eval import Evaluator, NERAcc, NERSpanRecognitionPR, PrunerPR, CorefBCubed, CorefMUC, CorefCeafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_bench = Evaluator(\n",
    "    predict_fn = model.pred_with_labels,\n",
    "    dataset_partial = valid_ds,\n",
    "    metrics = [NERAcc(), NERSpanRecognitionPR(), PrunerPR(), CorefBCubed(), CorefMUC(), CorefCeafe()],\n",
    "    device = 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_bench.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Eval for Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     25,
     40,
     44
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def b_cubed(clusters, mention_to_gold):\n",
    "    num, dem = 0, 0\n",
    "\n",
    "    for c in clusters:\n",
    "        if len(c) == 1:\n",
    "            continue\n",
    "\n",
    "        gold_counts = Counter()\n",
    "        correct = 0\n",
    "        for m in c:\n",
    "            if m in mention_to_gold:\n",
    "                gold_counts[tuple(mention_to_gold[m])] += 1\n",
    "        for c2, count in gold_counts.items():\n",
    "            if len(c2) != 1:\n",
    "                correct += count * count\n",
    "\n",
    "        num += correct / float(len(c))\n",
    "        dem += len(c)\n",
    "\n",
    "    return num, dem\n",
    "\n",
    "\n",
    "def muc(clusters, mention_to_gold):\n",
    "    tp, p = 0, 0\n",
    "    for c in clusters:\n",
    "        p += len(c) - 1\n",
    "        tp += len(c)\n",
    "        linked = set()\n",
    "        for m in c:\n",
    "            if m in mention_to_gold:\n",
    "                linked.add(mention_to_gold[m])\n",
    "            else:\n",
    "                tp -= 1\n",
    "        tp -= len(linked)\n",
    "    return tp, p\n",
    "\n",
    "\n",
    "def phi4(c1, c2):\n",
    "    return 2 * len([m for m in c1 if m in c2]) / float(len(c1) + len(c2))\n",
    "\n",
    "\n",
    "def ceafe(clusters, gold_clusters):\n",
    "    clusters = [c for c in clusters if len(c) != 1]\n",
    "    scores = np.zeros((len(gold_clusters), len(clusters)))\n",
    "    for i in range(len(gold_clusters)):\n",
    "        for j in range(len(clusters)):\n",
    "            scores[i, j] = phi4(gold_clusters[i], clusters[j])\n",
    "    matching = linear_assignment(-scores)\n",
    "    similarity = sum(scores[matching[0], matching[1]])\n",
    "\n",
    "    # similarity = sum(scores[matching[:, 0], matching[:, 1]])\n",
    "    return similarity, len(clusters), similarity, len(gold_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, instance in enumerate(dl):\n",
    "    outputs = model.pred_with_labels(**instance)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "instance['coref'].keys(), outputs['coref'].keys(), outputs.keys(), outputs['coref']['eval'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('=None, '.join(['clusters', 'gold_clusters', 'mention_to_predicted', 'mention_to_gold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ll =  outputs['coref']['eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ceafe(ll['clusters'], ll['gold_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phi4(ll['clusters'], ll['gold_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "muc(ll['clusters'], ll['mention_to_gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b_cubed(ll['clusters'], ll['mention_to_gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
