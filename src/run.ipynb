{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing paths from /home/priyansh/Dev/research/coref/mtl/src\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import click\n",
    "import torch\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import List, Callable, Dict, Iterable\n",
    "\n",
    "# Local imports\n",
    "try:\n",
    "    import _pathfix\n",
    "except ImportError:\n",
    "    from . import _pathfix\n",
    "from loops import training_loop\n",
    "from config import LOCATIONS as LOC, CONFIG\n",
    "from models.multitask import BasicMTL\n",
    "from dataiter import MultiTaskDataset\n",
    "# from eval import ner_all, ner_only_annotated, ner_span_recog_recall, ner_span_recog_precision, \\\n",
    "#     pruner_p, pruner_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15G        7,6G        4,1G        1,0G        3,6G        6,4G\r\n",
      "Swap:          979M         41M        938M\r\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_optimizer(model, optimizer_class: Callable, lr: float, freeze_encoder: bool):\n",
    "    if freeze_encoder:\n",
    "        return optimizer_class(\n",
    "            [param for name, param in model.named_parameters() if not name.startswith(\"encoder\")],\n",
    "            lr=lr\n",
    "        )\n",
    "    else:\n",
    "        return optimizer_class(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def get_pretrained_dirs(nm: str):\n",
    "    \"\"\"Check if the given nm is stored locally. If so, load that. Else, pass it on as is.\"\"\"\n",
    "    plausible_parent_dir: Path = LOC.root / \"models\" / \"huggingface\" / nm\n",
    "\n",
    "    if (\n",
    "            (plausible_parent_dir / \"config\").exists()\n",
    "            and (plausible_parent_dir / \"tokenizer\").exists()\n",
    "            and (plausible_parent_dir / \"encoder\").exists()\n",
    "    ):\n",
    "        return (\n",
    "            str(plausible_parent_dir / \"config\"),\n",
    "            str(plausible_parent_dir / \"tokenizer\"),\n",
    "            str(plausible_parent_dir / \"encoder\"),\n",
    "        )\n",
    "    else:\n",
    "        return nm, nm, nm\n",
    "\n",
    "\n",
    "def compute_metrics(metrics: Dict[str, Callable], logits, labels) -> Dict[str, float]:\n",
    "    return {metric_nm: metric_fn(logits=logits, labels=labels).item() for metric_nm, metric_fn in metrics.items()}\n",
    "\n",
    "\n",
    "def aggregate_metrics(inter_epoch: dict, intra_epoch: dict):\n",
    "    for task_nm in inter_epoch.keys():\n",
    "        for metric_nm, metric_list in intra_epoch[task_nm].items():\n",
    "            inter_epoch[task_nm][metric_nm].append(np.mean(metric_list))\n",
    "    return inter_epoch\n",
    "\n",
    "\n",
    "def simplest_loop(\n",
    "        epochs: int,\n",
    "        tasks: Iterable[str],\n",
    "        opt: torch.optim,\n",
    "        train_fn: Callable,\n",
    "        predict_fn: Callable,\n",
    "        trn_dl: Callable,\n",
    "        dev_dl: Callable,\n",
    "        eval_fns: dict,\n",
    ") -> (list, list, list):\n",
    "    train_loss = {task_nm: [] for task_nm in tasks}\n",
    "    train_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "    valid_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "\n",
    "    # Make data\n",
    "    trn_ds = trn_dl()\n",
    "    dev_ds = dev_dl()\n",
    "\n",
    "    # Epoch level\n",
    "    for e in range(epochs):\n",
    "\n",
    "        per_epoch_loss = {task_nm: [] for task_nm in tasks}\n",
    "        per_epoch_tr_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "        per_epoch_vl_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "\n",
    "        # Train\n",
    "        with Timer() as timer:\n",
    "\n",
    "            # Train Loop\n",
    "            for instance in tqdm(trn_ds):\n",
    "\n",
    "                # Reset the gradients.\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                outputs = train_fn(**instance)\n",
    "\n",
    "                \"\"\"\n",
    "                    Depending on instance.tasks list, do the following:\n",
    "                        - task specific loss (added to losses)\n",
    "                        - task specific metrics (added to metrics)\n",
    "                \"\"\"\n",
    "                for task_nm in instance['tasks']:\n",
    "                    loss = outputs[\"loss\"][task_nm]\n",
    "                    per_epoch_loss[task_nm].append(loss.item())\n",
    "\n",
    "                    # TODO: add other metrics here\n",
    "                    instance_metrics = compute_metrics(eval_fns[task_nm],\n",
    "                                                       logits=outputs[task_nm][\"logits\"],\n",
    "                                                       labels=outputs[task_nm][\"labels\"])\n",
    "                    for metric_nm, metric_vl in instance_metrics.items():\n",
    "                        per_epoch_tr_metrics[task_nm][metric_nm].append(metric_vl)\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # Val\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for instance in tqdm(dev_ds):\n",
    "                    outputs = predict_fn(**instance)\n",
    "\n",
    "                    for task_nm in instance[\"tasks\"]:\n",
    "                        logits = outputs[task_nm][\"logits\"]\n",
    "                        # TODO: make the label puller task specific somehow\n",
    "                        labels = instance[\"ner\"][\"gold_labels\"]\n",
    "\n",
    "                        instance_metrics = compute_metrics(eval_fns[task_nm], logits=logits, labels=labels)\n",
    "                        for metric_nm, metric_vl in instance_metrics.items():\n",
    "                            per_epoch_vl_metrics[task_nm][metric_nm].append(metric_vl)\n",
    "\n",
    "        # Bookkeep\n",
    "        for task_nm in tasks:\n",
    "            train_loss[task_nm].append(np.mean(per_epoch_loss[task_nm]))\n",
    "            train_metrics = aggregate_metrics(train_metrics, per_epoch_tr_metrics)\n",
    "            valid_metrics = aggregate_metrics(valid_metrics, per_epoch_vl_metrics)\n",
    "\n",
    "        print(f\"\\nEpoch: {e:3d}\" +\n",
    "              ''.join([f\" | {task_nm} Loss: {float(np.mean(per_epoch_loss[task_nm])):.5f}\" +\n",
    "                       ''.join([f\" | {task_nm} Tr_{metric_nm}: {float(metric_vls[-1]):.3f}\"\n",
    "                                for metric_nm, metric_vls in train_metrics[task_nm].items()]) +\n",
    "                       ''.join([f\" | {task_nm} Vl_{metric_nm}: {float(metric_vls[-1]):.3f}\"\n",
    "                                for metric_nm, metric_vls in valid_metrics[task_nm].items()])\n",
    "                       # f\" | {task_nm} Tr_c: {float(np.mean(per_epoch_tr_acc[task_nm])):.5f}\" +\n",
    "                       # f\" | {task_nm} Vl_c: {float(np.mean(per_epoch_vl_acc[task_nm])):.5f}\"\n",
    "                       for task_nm in tasks]))\n",
    "\n",
    "    return train_metrics, valid_metrics, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: str = 'ontonotes'\n",
    "epochs: int = 10\n",
    "encoder: str = \"bert-base-uncased\"\n",
    "tasks: List[str] = ('coref', 'ner', 'pruner')\n",
    "device: str = \"cpu\"\n",
    "trim: bool = True\n",
    "train_encoder: bool = False,\n",
    "ner_unweighted: bool = False\n",
    "filter_candidates_pos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 318 instances from ../data/parsed/ontonotes/development/MultiTaskDatasetDump_coref_ner_pruner.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyansh/Dev/research/coref/mtl/src/dataiter.py:83: UserWarning: The dataset has been trimmed to only 50 instances. This is NOT a legit experiment any more!\n",
      "  warnings.warn(\"The dataset has been trimmed to only 50 instances. This is NOT a legit experiment any more!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"binary_hdim\": 2000,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"coref_dropout\": 0.3,\n",
      "  \"device\": \"cpu\",\n",
      "  \"epochs\": 10,\n",
      "  \"filter_candidates_pos_threshold\": 2000,\n",
      "  \"freeze_encoder\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_span_width\": 5,\n",
      "  \"max_top_antecedents\": 50,\n",
      "  \"metadata_feature_size\": 20,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"ner_class_weights\": [\n",
      "    0.05324240542798054,\n",
      "    27.375502392344497,\n",
      "    34.064539175994284,\n",
      "    57.032296650717704,\n",
      "    23.163886639676114,\n",
      "    47.95072075092189,\n",
      "    112.36213668499607,\n",
      "    1882.0657894736842,\n",
      "    43.01864661654135,\n",
      "    193.03238866396762,\n",
      "    752.8263157894737,\n",
      "    150.56526315789475,\n",
      "    156.83881578947367,\n",
      "    114.06459330143541,\n",
      "    752.8263157894737,\n",
      "    1882.0657894736842,\n",
      "    203.4665718349929,\n",
      "    470.51644736842104,\n",
      "    1505.6526315789474\n",
      "  ],\n",
      "  \"ner_ignore_weights\": false,\n",
      "  \"ner_n_classes\": 19,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"pruner_class_weights\": [\n",
      "    0.5092894579428604,\n",
      "    27.412226906860866\n",
      "  ],\n",
      "  \"top_span_ratio\": 0.4,\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"trim\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unary_hdim\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": \"../models/huggingface/bert-base-uncased/config\"\n",
      "}\n",
      "\n",
      "Training commences!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_config, dir_tokenizer, dir_encoder = get_pretrained_dirs(encoder)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(dir_tokenizer)\n",
    "config = transformers.BertConfig(dir_config)\n",
    "config.max_span_width = 5\n",
    "config.coref_dropout = 0.3\n",
    "config.metadata_feature_size = 20\n",
    "config.unary_hdim = 1000\n",
    "config.binary_hdim = 2000\n",
    "config.top_span_ratio = 0.4\n",
    "config.max_top_antecedents = 50\n",
    "config.device = device\n",
    "config.epochs = epochs\n",
    "config.trim = trim\n",
    "config.freeze_encoder = not train_encoder\n",
    "config.ner_ignore_weights = ner_unweighted\n",
    "config.filter_candidates_pos_threshold = CONFIG['filter_candidates_pos_threshold'] \\\n",
    "    if filter_candidates_pos else -1\n",
    "\n",
    "\n",
    "if 'ner' in tasks or 'pruner' in tasks:\n",
    "    # Need to figure out the number of classes. Load a DL. Get the number. Delete the DL.\n",
    "    temp_ds = MultiTaskDataset(\n",
    "        src=dataset,\n",
    "        config=config,\n",
    "        tasks=tasks,\n",
    "        split=\"development\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    if 'ner' in tasks:\n",
    "        config.ner_n_classes = deepcopy(temp_ds.ner_tag_dict.__len__())\n",
    "        config.ner_class_weights = temp_ds.estimate_class_weights('ner')\n",
    "    else:\n",
    "        config.ner_n_classes = 1\n",
    "        config.ner_class_weights = [1.0, ]\n",
    "    if 'pruner' in tasks:\n",
    "        config.pruner_class_weights = temp_ds.estimate_class_weights('pruner')\n",
    "    del temp_ds\n",
    "else:\n",
    "    config.ner_n_classes = 1\n",
    "    config.ner_class_weights = [1.0, ]\n",
    "\n",
    "# Make the model\n",
    "model = BasicMTL(dir_encoder, config=config)\n",
    "\n",
    "# Load the data\n",
    "train_ds = partial(\n",
    "    MultiTaskDataset,\n",
    "    src=dataset,\n",
    "    config=config,\n",
    "    tasks=tasks,\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "valid_ds = partial(\n",
    "    MultiTaskDataset,\n",
    "    src=dataset,\n",
    "    config=config,\n",
    "    tasks=tasks,\n",
    "    split=\"development\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Make the optimizer\n",
    "opt = make_optimizer(model=model, optimizer_class=torch.optim.SGD, lr=0.005, freeze_encoder=config.freeze_encoder)\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Make the evaluation suite (may compute multiple metrics corresponding to the tasks)\n",
    "# eval_fns: Dict[str, Dict[str, Callable]] = {\n",
    "#     'ner': {'acc': ner_all,\n",
    "#             'acc_l': ner_only_annotated,\n",
    "#             'span_p': ner_span_recog_precision,\n",
    "#             'span_r': ner_span_recog_recall},\n",
    "#     'coref': {\n",
    "\n",
    "#     },\n",
    "#     'pruner': {'p': pruner_p,\n",
    "#                'r': pruner_r}\n",
    "# }\n",
    "\n",
    "print(config)\n",
    "print(\"Training commences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 2455 instances from ../data/parsed/ontonotes/train/MultiTaskDatasetDump_coref_ner_pruner.pkl.\n"
     ]
    }
   ],
   "source": [
    "dl = train_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import Evaluator, NERAcc, NERSpanRecognitionPR, PrunerPR, CorefBCubed, CorefMUC, CorefCeafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bench = Evaluator(\n",
    "    predict_fn = model.pred_with_labels,\n",
    "    dataset_partial = valid_ds,\n",
    "    metrics = [NERAcc(), NERSpanRecognitionPR(), PrunerPR(), CorefBCubed(), CorefMUC(), CorefCeafe()],\n",
    "    device = 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 318 instances from ../data/parsed/ontonotes/development/MultiTaskDatasetDump_coref_ner_pruner.pkl.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125fa541646e4a31908a667e99fff885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ner': {'acc': 0.007823196239769459,\n",
       "  'acc_nonzero': 0.04385122284293175,\n",
       "  'spanrec_p': 0.9928682446479797,\n",
       "  'spanrec_r': 0.01288938894867897},\n",
       " 'pruner': {'p': 0.09841908514499664, 'r': 0.019403498619794846},\n",
       " '': {'b_cubed_p': 0,\n",
       "  'b_cubed_r': 0,\n",
       "  'b_cubed_f1': 0,\n",
       "  'muc_p': 0,\n",
       "  'muc_r': 0,\n",
       "  'muc_f1': 0},\n",
       " 'coref': {'ceafe_p': 0.021555429666200575,\n",
       "  'ceafe_r': 0.011115397804372358,\n",
       "  'ceafe_f1': 0.014667346629025368}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bench.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval for Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     25,
     40,
     44
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def b_cubed(clusters, mention_to_gold):\n",
    "    num, dem = 0, 0\n",
    "\n",
    "    for c in clusters:\n",
    "        if len(c) == 1:\n",
    "            continue\n",
    "\n",
    "        gold_counts = Counter()\n",
    "        correct = 0\n",
    "        for m in c:\n",
    "            if m in mention_to_gold:\n",
    "                gold_counts[tuple(mention_to_gold[m])] += 1\n",
    "        for c2, count in gold_counts.items():\n",
    "            if len(c2) != 1:\n",
    "                correct += count * count\n",
    "\n",
    "        num += correct / float(len(c))\n",
    "        dem += len(c)\n",
    "\n",
    "    return num, dem\n",
    "\n",
    "\n",
    "def muc(clusters, mention_to_gold):\n",
    "    tp, p = 0, 0\n",
    "    for c in clusters:\n",
    "        p += len(c) - 1\n",
    "        tp += len(c)\n",
    "        linked = set()\n",
    "        for m in c:\n",
    "            if m in mention_to_gold:\n",
    "                linked.add(mention_to_gold[m])\n",
    "            else:\n",
    "                tp -= 1\n",
    "        tp -= len(linked)\n",
    "    return tp, p\n",
    "\n",
    "\n",
    "def phi4(c1, c2):\n",
    "    return 2 * len([m for m in c1 if m in c2]) / float(len(c1) + len(c2))\n",
    "\n",
    "\n",
    "def ceafe(clusters, gold_clusters):\n",
    "    clusters = [c for c in clusters if len(c) != 1]\n",
    "    scores = np.zeros((len(gold_clusters), len(clusters)))\n",
    "    for i in range(len(gold_clusters)):\n",
    "        for j in range(len(clusters)):\n",
    "            scores[i, j] = phi4(gold_clusters[i], clusters[j])\n",
    "    matching = linear_assignment(-scores)\n",
    "    similarity = sum(scores[matching[0], matching[1]])\n",
    "\n",
    "    # similarity = sum(scores[matching[:, 0], matching[:, 1]])\n",
    "    return similarity, len(clusters), similarity, len(gold_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, instance in enumerate(dl):\n",
    "    outputs = model.pred_with_labels(**instance)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance['coref'].keys(), outputs['coref'].keys(), outputs.keys(), outputs['coref']['eval'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=None, '.join(['clusters', 'gold_clusters', 'mention_to_predicted', 'mention_to_gold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll =  outputs['coref']['eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceafe(ll['clusters'], ll['gold_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi4(ll['clusters'], ll['gold_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muc(ll['clusters'], ll['mention_to_gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cubed(ll['clusters'], ll['mention_to_gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
