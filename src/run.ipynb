{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing paths from /home/priyansh/Dev/research/coref/mtl/src\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import click\n",
    "import torch\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import List, Callable, Dict, Iterable\n",
    "\n",
    "# Local imports\n",
    "try:\n",
    "    import _pathfix\n",
    "except ImportError:\n",
    "    from . import _pathfix\n",
    "from loops import training_loop\n",
    "from config import LOCATIONS as LOC, CONFIG\n",
    "from models.multitask import BasicMTL\n",
    "from dataiter import MultiTaskDataset\n",
    "from eval import ner_all, ner_only_annotated, ner_span_recog_recall, ner_span_recog_precision, \\\n",
    "    pruner_p, pruner_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_optimizer(model, optimizer_class: Callable, lr: float, freeze_encoder: bool):\n",
    "    if freeze_encoder:\n",
    "        return optimizer_class(\n",
    "            [param for name, param in model.named_parameters() if not name.startswith(\"encoder\")],\n",
    "            lr=lr\n",
    "        )\n",
    "    else:\n",
    "        return optimizer_class(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def get_pretrained_dirs(nm: str):\n",
    "    \"\"\"Check if the given nm is stored locally. If so, load that. Else, pass it on as is.\"\"\"\n",
    "    plausible_parent_dir: Path = LOC.root / \"models\" / \"huggingface\" / nm\n",
    "\n",
    "    if (\n",
    "            (plausible_parent_dir / \"config\").exists()\n",
    "            and (plausible_parent_dir / \"tokenizer\").exists()\n",
    "            and (plausible_parent_dir / \"encoder\").exists()\n",
    "    ):\n",
    "        return (\n",
    "            str(plausible_parent_dir / \"config\"),\n",
    "            str(plausible_parent_dir / \"tokenizer\"),\n",
    "            str(plausible_parent_dir / \"encoder\"),\n",
    "        )\n",
    "    else:\n",
    "        return nm, nm, nm\n",
    "\n",
    "\n",
    "def compute_metrics(metrics: Dict[str, Callable], logits, labels) -> Dict[str, float]:\n",
    "    return {metric_nm: metric_fn(logits=logits, labels=labels).item() for metric_nm, metric_fn in metrics.items()}\n",
    "\n",
    "\n",
    "def aggregate_metrics(inter_epoch: dict, intra_epoch: dict):\n",
    "    for task_nm in inter_epoch.keys():\n",
    "        for metric_nm, metric_list in intra_epoch[task_nm].items():\n",
    "            inter_epoch[task_nm][metric_nm].append(np.mean(metric_list))\n",
    "    return inter_epoch\n",
    "\n",
    "\n",
    "def simplest_loop(\n",
    "        epochs: int,\n",
    "        tasks: Iterable[str],\n",
    "        opt: torch.optim,\n",
    "        train_fn: Callable,\n",
    "        predict_fn: Callable,\n",
    "        trn_dl: Callable,\n",
    "        dev_dl: Callable,\n",
    "        eval_fns: dict,\n",
    ") -> (list, list, list):\n",
    "    train_loss = {task_nm: [] for task_nm in tasks}\n",
    "    train_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "    valid_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "\n",
    "    # Make data\n",
    "    trn_ds = trn_dl()\n",
    "    dev_ds = dev_dl()\n",
    "\n",
    "    # Epoch level\n",
    "    for e in range(epochs):\n",
    "\n",
    "        per_epoch_loss = {task_nm: [] for task_nm in tasks}\n",
    "        per_epoch_tr_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "        per_epoch_vl_metrics = {task_nm: {metric_nm: [] for metric_nm in eval_fns[task_nm].keys()} for task_nm in tasks}\n",
    "\n",
    "        # Train\n",
    "        with Timer() as timer:\n",
    "\n",
    "            # Train Loop\n",
    "            for instance in tqdm(trn_ds):\n",
    "\n",
    "                # Reset the gradients.\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                outputs = train_fn(**instance)\n",
    "\n",
    "                \"\"\"\n",
    "                    Depending on instance.tasks list, do the following:\n",
    "                        - task specific loss (added to losses)\n",
    "                        - task specific metrics (added to metrics)\n",
    "                \"\"\"\n",
    "                for task_nm in instance['tasks']:\n",
    "                    loss = outputs[\"loss\"][task_nm]\n",
    "                    per_epoch_loss[task_nm].append(loss.item())\n",
    "\n",
    "                    # TODO: add other metrics here\n",
    "                    instance_metrics = compute_metrics(eval_fns[task_nm],\n",
    "                                                       logits=outputs[task_nm][\"logits\"],\n",
    "                                                       labels=outputs[task_nm][\"labels\"])\n",
    "                    for metric_nm, metric_vl in instance_metrics.items():\n",
    "                        per_epoch_tr_metrics[task_nm][metric_nm].append(metric_vl)\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # Val\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for instance in tqdm(dev_ds):\n",
    "                    outputs = predict_fn(**instance)\n",
    "\n",
    "                    for task_nm in instance[\"tasks\"]:\n",
    "                        logits = outputs[task_nm][\"logits\"]\n",
    "                        # TODO: make the label puller task specific somehow\n",
    "                        labels = instance[\"ner\"][\"gold_labels\"]\n",
    "\n",
    "                        instance_metrics = compute_metrics(eval_fns[task_nm], logits=logits, labels=labels)\n",
    "                        for metric_nm, metric_vl in instance_metrics.items():\n",
    "                            per_epoch_vl_metrics[task_nm][metric_nm].append(metric_vl)\n",
    "\n",
    "        # Bookkeep\n",
    "        for task_nm in tasks:\n",
    "            train_loss[task_nm].append(np.mean(per_epoch_loss[task_nm]))\n",
    "            train_metrics = aggregate_metrics(train_metrics, per_epoch_tr_metrics)\n",
    "            valid_metrics = aggregate_metrics(valid_metrics, per_epoch_vl_metrics)\n",
    "\n",
    "        print(f\"\\nEpoch: {e:3d}\" +\n",
    "              ''.join([f\" | {task_nm} Loss: {float(np.mean(per_epoch_loss[task_nm])):.5f}\" +\n",
    "                       ''.join([f\" | {task_nm} Tr_{metric_nm}: {float(metric_vls[-1]):.3f}\"\n",
    "                                for metric_nm, metric_vls in train_metrics[task_nm].items()]) +\n",
    "                       ''.join([f\" | {task_nm} Vl_{metric_nm}: {float(metric_vls[-1]):.3f}\"\n",
    "                                for metric_nm, metric_vls in valid_metrics[task_nm].items()])\n",
    "                       # f\" | {task_nm} Tr_c: {float(np.mean(per_epoch_tr_acc[task_nm])):.5f}\" +\n",
    "                       # f\" | {task_nm} Vl_c: {float(np.mean(per_epoch_vl_acc[task_nm])):.5f}\"\n",
    "                       for task_nm in tasks]))\n",
    "\n",
    "    return train_metrics, valid_metrics, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: str = 'ontonotes'\n",
    "epochs: int = 10\n",
    "encoder: str = \"bert-base-uncased\"\n",
    "tasks: List[str] = ('coref',)\n",
    "device: str = \"cpu\"\n",
    "trim: bool = False\n",
    "train_encoder: bool = False,\n",
    "ner_unweighted: bool = False\n",
    "filter_candidates_pos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"binary_hdim\": 2000,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"coref_dropout\": 0.3,\n",
      "  \"device\": \"cpu\",\n",
      "  \"epochs\": 10,\n",
      "  \"filter_candidates_pos_threshold\": 2000,\n",
      "  \"freeze_encoder\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_span_width\": 5,\n",
      "  \"max_top_antecedents\": 50,\n",
      "  \"metadata_feature_size\": 20,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"ner_class_weights\": [\n",
      "    1.0\n",
      "  ],\n",
      "  \"ner_ignore_weights\": false,\n",
      "  \"ner_n_classes\": 1,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"top_span_ratio\": 0.4,\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"trim\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unary_hdim\": 1000,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": \"../models/huggingface/bert-base-uncased/config\"\n",
      "}\n",
      "\n",
      "Training commences!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_config, dir_tokenizer, dir_encoder = get_pretrained_dirs(encoder)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(dir_tokenizer)\n",
    "config = transformers.BertConfig(dir_config)\n",
    "config.max_span_width = 5\n",
    "config.coref_dropout = 0.3\n",
    "config.metadata_feature_size = 20\n",
    "config.unary_hdim = 1000\n",
    "config.binary_hdim = 2000\n",
    "config.top_span_ratio = 0.4\n",
    "config.max_top_antecedents = 50\n",
    "config.device = device\n",
    "config.epochs = epochs\n",
    "config.trim = trim\n",
    "config.freeze_encoder = not train_encoder\n",
    "config.ner_ignore_weights = ner_unweighted\n",
    "config.filter_candidates_pos_threshold = CONFIG['filter_candidates_pos_threshold'] \\\n",
    "    if filter_candidates_pos else -1\n",
    "\n",
    "\n",
    "if 'ner' in tasks or 'pruner' in tasks:\n",
    "    # Need to figure out the number of classes. Load a DL. Get the number. Delete the DL.\n",
    "    temp_ds = MultiTaskDataset(\n",
    "        src=dataset,\n",
    "        config=config,\n",
    "        tasks=tasks,\n",
    "        split=\"development\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    if 'ner' in tasks:\n",
    "        config.ner_n_classes = deepcopy(temp_ds.ner_tag_dict.__len__())\n",
    "        config.ner_class_weights = temp_ds.estimate_class_weights('ner')\n",
    "    else:\n",
    "        config.ner_n_classes = 1\n",
    "        config.ner_class_weights = [1.0, ]\n",
    "    if 'pruner' in tasks:\n",
    "        config.pruner_class_weights = temp_ds.estimate_class_weights('pruner')\n",
    "    del temp_ds\n",
    "else:\n",
    "    config.ner_n_classes = 1\n",
    "    config.ner_class_weights = [1.0, ]\n",
    "\n",
    "# Make the model\n",
    "model = BasicMTL(dir_encoder, config=config)\n",
    "\n",
    "# Load the data\n",
    "train_ds = partial(\n",
    "    MultiTaskDataset,\n",
    "    src=dataset,\n",
    "    config=config,\n",
    "    tasks=tasks,\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "valid_ds = partial(\n",
    "    MultiTaskDataset,\n",
    "    src=dataset,\n",
    "    config=config,\n",
    "    tasks=tasks,\n",
    "    split=\"development\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Make the optimizer\n",
    "opt = make_optimizer(model=model, optimizer_class=torch.optim.SGD, lr=0.005, freeze_encoder=config.freeze_encoder)\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Make the evaluation suite (may compute multiple metrics corresponding to the tasks)\n",
    "eval_fns: Dict[str, Dict[str, Callable]] = {\n",
    "    'ner': {'acc': ner_all,\n",
    "            'acc_l': ner_only_annotated,\n",
    "            'span_p': ner_span_recog_precision,\n",
    "            'span_r': ner_span_recog_recall},\n",
    "    'coref': {\n",
    "\n",
    "    },\n",
    "    'pruner': {'p': pruner_p,\n",
    "               'r': pruner_r}\n",
    "}\n",
    "\n",
    "print(config)\n",
    "print(\"Training commences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 2775 instances from ../data/parsed/ontonotes/train/MultiTaskDatasetDump_coref.pkl.\n"
     ]
    }
   ],
   "source": [
    "dl = train_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval for Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def b_cubed(clusters, mention_to_gold):\n",
    "    num, dem = 0, 0\n",
    "\n",
    "    for c in clusters:\n",
    "        if len(c) == 1:\n",
    "            continue\n",
    "\n",
    "        gold_counts = Counter()\n",
    "        correct = 0\n",
    "        for m in c:\n",
    "            if m in mention_to_gold:\n",
    "                gold_counts[tuple(mention_to_gold[m])] += 1\n",
    "        for c2, count in gold_counts.items():\n",
    "            if len(c2) != 1:\n",
    "                correct += count * count\n",
    "\n",
    "        num += correct / float(len(c))\n",
    "        dem += len(c)\n",
    "\n",
    "    return num, dem\n",
    "\n",
    "\n",
    "def muc(clusters, mention_to_gold):\n",
    "    tp, p = 0, 0\n",
    "    for c in clusters:\n",
    "        p += len(c) - 1\n",
    "        tp += len(c)\n",
    "        linked = set()\n",
    "        for m in c:\n",
    "            if m in mention_to_gold:\n",
    "                linked.add(mention_to_gold[m])\n",
    "            else:\n",
    "                tp -= 1\n",
    "        tp -= len(linked)\n",
    "    return tp, p\n",
    "\n",
    "\n",
    "def phi4(c1, c2):\n",
    "    return 2 * len([m for m in c1 if m in c2]) / float(len(c1) + len(c2))\n",
    "\n",
    "\n",
    "def ceafe(clusters, gold_clusters):\n",
    "    clusters = [c for c in clusters if len(c) != 1]\n",
    "    scores = np.zeros((len(gold_clusters), len(clusters)))\n",
    "    for i in range(len(gold_clusters)):\n",
    "        for j in range(len(clusters)):\n",
    "            scores[i, j] = phi4(gold_clusters[i], clusters[j])\n",
    "    matching = linear_assignment(-scores)\n",
    "    similarity = sum(scores[matching[0], matching[1]])\n",
    "\n",
    "    # similarity = sum(scores[matching[:, 0], matching[:, 1]])\n",
    "    return similarity, len(clusters), similarity, len(gold_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, instance in enumerate(dl):\n",
    "    outputs = model.pred_with_labels(**instance)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15G         11G        1,4G        827M        2,3G        2,6G\r\n",
      "Swap:          979M        759M        220M\r\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['gold_cluster_ids_on_candidates', 'gold_starts', 'gold_ends', 'gold_cluster_ids']),\n",
       " dict_keys(['logits', 'labels']),\n",
       " dict_keys(['loss', 'ner', 'coref', 'pruner']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance['coref'].keys(), outputs['coref'].keys(), outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We go for each cluster id. That's not too difficult\n",
    "gold_clusters = {}\n",
    "for i, val in enumerate(instance['coref']['gold_cluster_ids']):\n",
    "    cluster_id = val.item()\n",
    "    \n",
    "    # Populate the dict\n",
    "    gold_clusters[cluster_id] = gold_clusters.get(cluster_id, []) + \\\n",
    "        [(instance['coref']['gold_starts'][i].item(), instance['coref']['gold_ends'][i].item())]\n",
    "\n",
    "gold_clusters = [tuple(v) for v in gold_clusters.values()]\n",
    "mention_to_gold = {}\n",
    "for c in gold_clusters:\n",
    "    for mention in c:\n",
    "        mention_to_gold[mention] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, torch.Size([96]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_clusters.__len__(), instance['coref']['gold_starts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = torch.argmax(outputs['coref']['top_antecedent_scores'], dim=1, keepdim=False)\n",
    "ids = outputs['input_ids']\n",
    "top_span_starts = outputs['pruner']['top_span_starts']\n",
    "top_span_ends = outputs['pruner']['top_span_ends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['pruner'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = {name: tensor.cpu() for name, tensor in example.items()} \n",
    "# outputs = {name: tensor.cpu() for name, tensor in outputs.items()}\n",
    "# gold_clusters = {}\n",
    "# for i in range(len(example[\"cluster_ids\"])):\n",
    "#     assert len(example[\"cluster_ids\"]) == len(\n",
    "#         example[\"gold_starts\"]) == len(example[\"gold_ends\"])\n",
    "#     cid = example[\"cluster_ids\"][i].item()\n",
    "#     if cid in gold_clusters:\n",
    "#         gold_clusters[cid].append((example[\"gold_starts\"][i].item(),\n",
    "#                                    example[\"gold_ends\"][i].item()))\n",
    "#     else:\n",
    "#         gold_clusters[cid] = [(example[\"gold_starts\"][i].item(),\n",
    "#                                example[\"gold_ends\"][i].item())]\n",
    "\n",
    "# gold_clusters = [tuple(v) for v in gold_clusters.values()]\n",
    "# mention_to_gold = {}\n",
    "# for c in gold_clusters:\n",
    "#     for mention in c:\n",
    "#         mention_to_gold[mention] = c\n",
    "\n",
    "    top_indices = torch.argmax(outputs[\"top_antecedent_scores\"], dim=-1, keepdim=False)\n",
    "    ids = outputs[\"flattened_ids\"]\n",
    "    top_span_starts = outputs[\"top_span_starts\"]\n",
    "    top_span_ends = outputs[\"top_span_ends\"]\n",
    "    top_antecedents = outputs[\"top_antecedents\"]\n",
    "    mention_indices = []\n",
    "    antecedent_indices = []\n",
    "    predicted_antecedents = []\n",
    "    for i in range(len(outputs[\"top_span_ends\"])):\n",
    "        if top_indices[i] > 0:\n",
    "            mention_indices.append(i)\n",
    "            antecedent_indices.append(top_antecedents[i][top_indices[i] - 1].item())\n",
    "            predicted_antecedents.append(top_indices[i] - 1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
